<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title></title><link href='file://night/mermaid.dark.css' rel='stylesheet' type='text/css' /><link href='file://night/codeblock.dark.css' rel='stylesheet' type='text/css' /><link href='file://night/sourcemode.dark.css' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 40px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 2; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px !important; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 32px; padding-right: 32px; padding-bottom: 0px; break-after: avoid; }
  .typora-export #write::after { height: 0px; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.html-for-mac .inline-math-svg .MathJax_SVG { vertical-align: 0.2px; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none 0s ease 0s; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; margin-top: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="mermaid"] svg, [lang="flow"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
svg[id^="mermaidChart"] { line-height: 1em; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
mark .md-meta { color: rgb(0, 0, 0); opacity: 0.3 !important; }


/* Flowchart variables */
/* Sequence Diagram variables */
/* Gantt chart variables */
/* state colors */
.label {
  
  color: #333; }

.label text {
  fill: #333; }

.node rect,
.node circle,
.node ellipse,
.node polygon {
  fill: #BDD5EA;
  stroke: #9370DB;
  stroke-width: 1px; }

.node .label {
  text-align: center; }

.node.clickable {
  cursor: pointer; }

.arrowheadPath {
  fill: lightgrey; }

.edgePath .path {
  stroke: lightgrey;
  stroke-width: 1.5px; }

.edgeLabel {
  background-color: #e8e8e8;
  text-align: center; }

.cluster rect {
  fill: #6D6D65;
  stroke: rgba(255, 255, 255, 0.25);
  stroke-width: 1px; }

.cluster text {
  fill: #F9FFFE; }

div.mermaidTooltip {
  position: absolute;
  text-align: center;
  max-width: 200px;
  padding: 2px;
  
  font-size: 12px;
  background: #6D6D65;
  border: 1px solid rgba(255, 255, 255, 0.25);
  border-radius: 2px;
  pointer-events: none;
  z-index: 100; }

.actor {
  stroke: #81B1DB;
  fill: #BDD5EA; }

text.actor {
  fill: black;
  stroke: none; }

.actor-line {
  stroke: lightgrey; }

.messageLine0 {
  stroke-width: 1.5;
  stroke-dasharray: '2 2';
  stroke: lightgrey; }

.messageLine1 {
  stroke-width: 1.5;
  stroke-dasharray: '2 2';
  stroke: lightgrey; }

#arrowhead {
  fill: lightgrey; }

.sequenceNumber {
  fill: white; }

#sequencenumber {
  fill: lightgrey; }

#crosshead path {
  fill: lightgrey !important;
  stroke: lightgrey !important; }

.messageText {
  fill: lightgrey;
  stroke: none; }

.labelBox {
  stroke: #81B1DB;
  fill: #BDD5EA; }

.labelText {
  fill: #323D47;
  stroke: none; }

.loopText {
  fill: lightgrey;
  stroke: none; }

.loopLine {
  stroke-width: 2;
  stroke-dasharray: '2 2';
  stroke: #81B1DB; }

.note {
  stroke: rgba(255, 255, 255, 0.25);
  fill: #fff5ad; }

.noteText {
  fill: black;
  stroke: none;
  
  font-size: 14px; }

.activation0 {
  fill: #f4f4f4;
  stroke: #666; }

.activation1 {
  fill: #f4f4f4;
  stroke: #666; }

.activation2 {
  fill: #f4f4f4;
  stroke: #666; }

/** Section styling */
.section {
  stroke: none;
  opacity: 0.2; }

.section0 {
  fill: rgba(255, 255, 255, 0.3); }

.section2 {
  fill: #EAE8B9; }

.section1,
.section3 {
  fill: white;
  opacity: 0.2; }

.sectionTitle0 {
  fill: #F9FFFE; }

.sectionTitle1 {
  fill: #F9FFFE; }

.sectionTitle2 {
  fill: #F9FFFE; }

.sectionTitle3 {
  fill: #F9FFFE; }

.sectionTitle {
  text-anchor: start;
  font-size: 11px;
  text-height: 14px;
   }

/* Grid and axis */
.grid .tick {
  stroke: lightgrey;
  opacity: 0.3;
  shape-rendering: crispEdges; }

.grid path {
  stroke-width: 0; }

/* Today line */
.today {
  fill: none;
  stroke: #DB5757;
  stroke-width: 2px; }

/* Task styling */
/* Default task */
.task {
  stroke-width: 2; }

.taskText {
  text-anchor: middle;
   }

.taskText:not([font-size]) {
  font-size: 11px; }

.taskTextOutsideRight {
  fill: #323D47;
  text-anchor: start;
  font-size: 11px;
   }

.taskTextOutsideLeft {
  fill: #323D47;
  text-anchor: end;
  font-size: 11px; }

/* Special case clickable */
.task.clickable {
  cursor: pointer; }

.taskText.clickable {
  cursor: pointer;
  fill: #003163 !important;
  font-weight: bold; }

.taskTextOutsideLeft.clickable {
  cursor: pointer;
  fill: #003163 !important;
  font-weight: bold; }

.taskTextOutsideRight.clickable {
  cursor: pointer;
  fill: #003163 !important;
  font-weight: bold; }

/* Specific task settings for the sections*/
.taskText0,
.taskText1,
.taskText2,
.taskText3 {
  fill: #323D47; }

.task0,
.task1,
.task2,
.task3 {
  fill: #BDD5EA;
  stroke: rgba(255, 255, 255, 0.5); }

.taskTextOutside0,
.taskTextOutside2 {
  fill: lightgrey; }

.taskTextOutside1,
.taskTextOutside3 {
  fill: lightgrey; }

/* Active task */
.active0,
.active1,
.active2,
.active3 {
  fill: #81B1DB;
  stroke: rgba(255, 255, 255, 0.5); }

.activeText0,
.activeText1,
.activeText2,
.activeText3 {
  fill: #323D47 !important; }

/* Completed task */
.done0,
.done1,
.done2,
.done3 {
  stroke: grey;
  fill: lightgrey;
  stroke-width: 2; }

.doneText0,
.doneText1,
.doneText2,
.doneText3 {
  fill: #323D47 !important; }

/* Tasks on the critical line */
.crit0,
.crit1,
.crit2,
.crit3 {
  stroke: #E83737;
  fill: #E83737;
  stroke-width: 2; }

.activeCrit0,
.activeCrit1,
.activeCrit2,
.activeCrit3 {
  stroke: #E83737;
  fill: #81B1DB;
  stroke-width: 2; }

.doneCrit0,
.doneCrit1,
.doneCrit2,
.doneCrit3 {
  stroke: #E83737;
  fill: lightgrey;
  stroke-width: 2;
  cursor: pointer;
  shape-rendering: crispEdges; }

.milestone {
  transform: rotate(45deg) scale(0.8, 0.8); }

.milestoneText {
  font-style: italic; }

.doneCritText0,
.doneCritText1,
.doneCritText2,
.doneCritText3 {
  fill: #323D47 !important; }

.activeCritText0,
.activeCritText1,
.activeCritText2,
.activeCritText3 {
  fill: #323D47 !important; }

.titleText {
  text-anchor: middle;
  font-size: 18px;
  fill: #323D47;
   }

g.classGroup text {
  fill: #9370DB;
  stroke: none;
  
  font-size: 10px; }
  g.classGroup text .title {
    font-weight: bolder; }

g.classGroup rect {
  fill: #BDD5EA;
  stroke: #9370DB; }

g.classGroup line {
  stroke: #9370DB;
  stroke-width: 1; }

.classLabel .box {
  stroke: none;
  stroke-width: 0;
  fill: #BDD5EA;
  opacity: 0.5; }

.classLabel .label {
  fill: #9370DB;
  font-size: 10px; }

.relation {
  stroke: #9370DB;
  stroke-width: 1;
  fill: none; }

#compositionStart {
  fill: #9370DB;
  stroke: #9370DB;
  stroke-width: 1; }

#compositionEnd {
  fill: #9370DB;
  stroke: #9370DB;
  stroke-width: 1; }

#aggregationStart {
  fill: #BDD5EA;
  stroke: #9370DB;
  stroke-width: 1; }

#aggregationEnd {
  fill: #BDD5EA;
  stroke: #9370DB;
  stroke-width: 1; }

#dependencyStart {
  fill: #9370DB;
  stroke: #9370DB;
  stroke-width: 1; }

#dependencyEnd {
  fill: #9370DB;
  stroke: #9370DB;
  stroke-width: 1; }

#extensionStart {
  fill: #9370DB;
  stroke: #9370DB;
  stroke-width: 1; }

#extensionEnd {
  fill: #9370DB;
  stroke: #9370DB;
  stroke-width: 1; }

.commit-id,
.commit-msg,
.branch-label {
  fill: lightgrey;
  color: lightgrey;
   }

.pieTitleText {
  text-anchor: middle;
  font-size: 25px;
  fill: #323D47;
   }

.slice {
   }

g.stateGroup text {
  fill: #eee;
  stroke: none;
  font-size: 10px;
   }

g.stateGroup circle {
  fill: white !important;
  stroke: white !important;
}

g.stateGroup .state-title {
  font-weight: bolder;
  fill: black; }

g.stateGroup rect {
  fill: #ececff;
  stroke: #9370DB; }

g.stateGroup line {
  stroke: #9370DB;
  stroke-width: 1; }

.transition {
  stroke: #9370DB;
  stroke-width: 1;
  fill: none; }

.stateGroup .composit {
  fill: #555;
  border-bottom: 1px; }

.state-note {
  stroke: rgba(255, 255, 255, 0.25);
  fill: #fff5ad; }
  .state-note text {
    fill: black;
    stroke: none;
    font-size: 10px; }

.stateLabel .box {
  stroke: none;
  stroke-width: 0;
  fill: #BDD5EA;
  opacity: 0.5; }

.stateLabel text {
  fill: black;
  font-size: 10px;
  font-weight: bold;
   }

;
/* CSS Document */

/** code highlight */

.cm-s-inner .cm-variable,
.cm-s-inner .cm-operator,
.cm-s-inner .cm-property {
    color: #b8bfc6;
}

.cm-s-inner .cm-keyword {
    color: #C88FD0;
}

.cm-s-inner .cm-tag {
    color: #7DF46A;
}

.cm-s-inner .cm-attribute {
    color: #7575E4;
}

.CodeMirror div.CodeMirror-cursor {
    border-left: 1px solid #b8bfc6;
    z-index: 3;
}

.cm-s-inner .cm-string {
    color: #D26B6B;
}

.cm-s-inner .cm-comment,
.cm-s-inner.cm-comment {
    color: #DA924A;
}

.cm-s-inner .cm-header,
.cm-s-inner .cm-def,
.cm-s-inner.cm-header,
.cm-s-inner.cm-def {
    color: #8d8df0;
}

.cm-s-inner .cm-quote,
.cm-s-inner.cm-quote {
    color: #57ac57;
}

.cm-s-inner .cm-hr {
    color: #d8d5d5;
}

.cm-s-inner .cm-link {
    color: #d3d3ef;
}

.cm-s-inner .cm-negative {
    color: #d95050;
}

.cm-s-inner .cm-positive {
    color: #50e650;
}

.cm-s-inner .cm-string-2 {
    color: #f50;
}

.cm-s-inner .cm-meta,
.cm-s-inner .cm-qualifier {
    color: #b7b3b3;
}

.cm-s-inner .cm-builtin {
    color: #f3b3f8;
}

.cm-s-inner .cm-bracket {
    color: #997;
}

.cm-s-inner .cm-atom,
.cm-s-inner.cm-atom {
    color: #84B6CB;
}

.cm-s-inner .cm-number {
    color: #64AB8F;
}

.cm-s-inner .cm-variable {
    color: #b8bfc6;
}

.cm-s-inner .cm-variable-2 {
    color: #9FBAD5;
}

.cm-s-inner .cm-variable-3 {
    color: #1cc685;
}

.CodeMirror-selectedtext,
.CodeMirror-selected {
    background: #4a89dc;
    color: #fff !important;
    text-shadow: none;
}

.CodeMirror-gutters {
    border-right: none;
}
;
/* CSS Document */

/** markdown source **/
.cm-s-typora-default .cm-header, 
.cm-s-typora-default .cm-property
{
    color: #cebcca;
}

.CodeMirror.cm-s-typora-default div.CodeMirror-cursor{
    border-left: 3px solid #b8bfc6;
}

.cm-s-typora-default .cm-comment {
    color: #9FB1FF;
}

.cm-s-typora-default .cm-string {
    color: #A7A7D9
}

.cm-s-typora-default .cm-atom, .cm-s-typora-default .cm-number {
    color: #848695;
    font-style: italic;
}

.cm-s-typora-default .cm-link {
    color: #95B94B;
}

.cm-s-typora-default .CodeMirror-activeline-background {
    background: rgba(51, 51, 51, 0.72);
}

.cm-s-typora-default .cm-comment, .cm-s-typora-default .cm-code {
	color: #8aa1e1;
}@import "";
@import "";
@import "";

:root {
    --bg-color:  #363B40;
    --side-bar-bg-color: #2E3033;
    --text-color: #b8bfc6;

    --select-text-bg-color:#4a89dc;

    --item-hover-bg-color: #0a0d16;
    --control-text-color: #b7b7b7;
    --control-text-hover-color: #eee;
    --window-border: 1px solid #555;

    --active-file-bg-color: rgb(34, 34, 34);
    --active-file-border-color: #8d8df0;

    --primary-color: #a3d5fe;

    --active-file-text-color: white;
    --item-hover-bg-color: #70717d;
    --item-hover-text-color: white;
    --primary-color: #6dc1e7;

    --rawblock-edit-panel-bd: #333;

    --search-select-bg-color: #428bca;
}

html {
    font-size: 16px;
}

html,
body {
    -webkit-text-size-adjust: 100%;
    -ms-text-size-adjust: 100%;
    background: #363B40;
    background: var(--bg-color);
    fill: currentColor;
    line-height: 1.625rem;
}

#write {
    max-width: 914px;
}

html,
body,
button,
input,
select,
textarea,
div.code-tooltip-content {
    color: #b8bfc6;
    border-color: transparent;
}

div.code-tooltip,
.md-hover-tip .md-arrow:after {
    background: #333;
}

.popover.bottom > .arrow:after {
    border-bottom-color: #333;
}

html,
body,
button,
input,
select,
textarea {
    font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}

hr {
    height: 2px;
    border: 0;
    margin: 24px 0 !important;
}

h1,
h2,
h3,
h4,
h5,
h6 {
    font-family: "Lucida Grande", "Corbel", sans-serif;
    font-weight: normal;
    clear: both;
    -ms-word-wrap: break-word;
    word-wrap: break-word;
    margin: 0;
    padding: 0;
    color: #DEDEDE
}

h1 {
    font-size: 2.5rem;
    /* 36px */
    line-height: 2.75rem;
    /* 40px */
    margin-bottom: 1.5rem;
    /* 24px */
    letter-spacing: -1.5px;
}

h2 {
    font-size: 1.63rem;
    /* 24px */
    line-height: 1.875rem;
    /* 30px */
    margin-bottom: 1.5rem;
    /* 24px */
    letter-spacing: -1px;
    font-weight: bold;
}

h3 {
    font-size: 1.17rem;
    /* 18px */
    line-height: 1.5rem;
    /* 24px */
    margin-bottom: 1.5rem;
    /* 24px */
    letter-spacing: -1px;
    font-weight: bold;
}

h4 {
    font-size: 1.12rem;
    /* 16px */
    line-height: 1.375rem;
    /* 22px */
    margin-bottom: 1.5rem;
    /* 24px */
    color: white;
}

h5 {
    font-size: 0.97rem;
    /* 16px */
    line-height: 1.25rem;
    /* 22px */
    margin-bottom: 1.5rem;
    /* 24px */
    font-weight: bold;
}

h6 {
    font-size: 0.93rem;
    /* 16px */
    line-height: 1rem;
    /* 16px */
    margin-bottom: 0.75rem;
    color: white;
}

@media (min-width: 980px) {
    h3.md-focus:before,
    h4.md-focus:before,
    h5.md-focus:before,
    h6.md-focus:before {
        color: #ddd;
        border: 1px solid #ddd;
        border-radius: 3px;
        position: absolute;
        left: -1.642857143rem;
        top: .357142857rem;
        float: left;
        font-size: 9px;
        padding-left: 2px;
        padding-right: 2px;
        vertical-align: bottom;
        font-weight: normal;
        line-height: normal;
    }

    h3.md-focus:before {
        content: 'h3';
    }

    h4.md-focus:before {
        content: 'h4';
    }

    h5.md-focus:before {
        content: 'h5';
        top: 0px;
    }

    h6.md-focus:before {
        content: 'h6';
        top: 0px;
    }
}

a {
    text-decoration: none;
    outline: 0;
}

a:hover {
    outline: 0;
}

a:focus {
    outline: thin dotted;
}

sup.md-footnote {
    background-color: #555;
    color: #ddd;
}

p {
    -ms-word-wrap: break-word;
    word-wrap: break-word;
}

p,
ul,
dd,
ol,
hr,
address,
pre,
table,
iframe,
.wp-caption,
.wp-audio-shortcode,
.wp-video-shortcode {
    margin-top: 0;
    margin-bottom: 1.5rem;
    /* 24px */
}

li > blockquote {
	margin-bottom: 0;
}

audio:not([controls]) {
    display: none;
}

[hidden] {
    display: none;
}

::-moz-selection {
    background: #4a89dc;
    color: #fff;
    text-shadow: none;
}

*.in-text-selection,
::selection {
    background: #4a89dc;
    color: #fff;
    text-shadow: none;
}

ul,
ol {
    padding: 0 0 0 1.875rem;
    /* 30px */
}

ul {
    list-style: square;
}

ol {
    list-style: decimal;
}

ul ul,
ol ol,
ul ol,
ol ul {
    margin: 0;
}

b,
th,
dt,
strong {
    font-weight: bold;
}

i,
em,
dfn,
cite {
    font-style: italic;
}

blockquote {
    padding-left: 1.875rem;
    margin: 0 0 1.875rem 1.875rem;
    border-left: solid 2px #474d54;
    padding-left: 30px;
    margin-top: 35px;
}

pre,
code,
kbd,
tt,
var {
    background: rgba(0, 0, 0, 0.05);
    font-size: 0.875rem;
    font-family: Monaco, Consolas, "Andale Mono", "DejaVu Sans Mono", monospace;
}

kbd {
    padding: 2px 4px;
    font-size: 90%;
    color: #fff;
    background-color: #333;
    border-radius: 3px;
    box-shadow: inset 0 -1px 0 rgba(0,0,0,.25);
}

pre.md-fences {
    padding: 10px 10px 10px 30px;
    margin-bottom: 20px;
    background: #333;
}

.CodeMirror-gutters {
    background: #333;
    border-right: 1px solid transparent;
}

.enable-diagrams pre.md-fences[lang="sequence"] .code-tooltip,
.enable-diagrams pre.md-fences[lang="flow"] .code-tooltip,
.enable-diagrams pre.md-fences[lang="mermaid"] .code-tooltip {
    bottom: -2.2em;
    right: 4px;
}

code,
kbd,
tt,
var {
    padding: 2px 5px;
}

table {
    max-width: 100%;
    width: 100%;
    border-collapse: collapse;
    border-spacing: 0;
}

th,
td {
    padding: 5px 10px;
    vertical-align: top;
}

a {
    -webkit-transition: all .2s ease-in-out;
    transition: all .2s ease-in-out;
}

hr {
    background: #474d54;
    /* variable */
}

h1 {
    margin-top: 2em;
}

a {
    color: #e0e0e0;
    text-decoration: underline;
}

a:hover {
    color: #fff;
}

.md-inline-math script {
    color: #81b1db;
}

b,
th,
dt,
strong {
    color: #DEDEDE;
    /* variable */
}

mark {
    background: #D3D40E;
}

blockquote {
    color: #9DA2A6;
}

table a {
    color: #DEDEDE;
    /* variable */
}

th,
td {
    border: solid 1px #474d54;
    /* variable */
}

.task-list {
    padding-left: 0;
}

.md-task-list-item {
    padding-left: 1.25rem;
}

.md-task-list-item > input {
    top: auto;
}

.md-task-list-item > input:before {
    content: "";
    display: inline-block;
    width: 0.875rem;
    height: 0.875rem;
    vertical-align: middle;
    text-align: center;
    border: 1px solid #b8bfc6;
    background-color: #363B40;
    margin-top: -0.4rem;
}

.md-task-list-item > input:checked:before,
.md-task-list-item > input[checked]:before {
    content: '\221A';
    /*◘*/
    font-size: 0.625rem;
    line-height: 0.625rem;
    color: #DEDEDE;
}

/** quick open **/
.auto-suggest-container {
    border: 0px;
    background-color: #525C65;
}

#typora-quick-open {
    background-color: #525C65;
}

#typora-quick-open input{
    background-color: #525C65;
    border: 0;
    border-bottom: 1px solid grey;
}

.typora-quick-open-item {
    background-color: inherit;
    color: inherit;
}

.typora-quick-open-item.active,
.typora-quick-open-item:hover {
    background-color: #4D8BDB;
    color: white;
}

.typora-quick-open-item:hover {
    background-color: rgba(77, 139, 219, 0.8);
}

.typora-search-spinner > div {
  background-color: #fff;
}

#write pre.md-meta-block {
    border-bottom: 1px dashed #ccc;
    background: transparent;
    padding-bottom: 0.6em;
    line-height: 1.6em;
}

.btn,
.btn .btn-default {
    background: transparent;
    color: #b8bfc6;
}

.ty-table-edit {
    border-top: 1px solid gray;
    background-color: #363B40;
}

.popover-title {
    background: transparent;
}

.md-image>.md-meta {
    color: #BBBBBB;
    background: transparent;
}

.md-expand.md-image>.md-meta {
    color: #DDD;
}

#write>h3:before,
#write>h4:before,
#write>h5:before,
#write>h6:before {
    border: none;
    border-radius: 0px;
    color: #888;
    text-decoration: underline;
    left: -1.4rem;
    top: 0.2rem;
}

#write>h3.md-focus:before {
    top: 2px;
}

#write>h4.md-focus:before {
    top: 2px;
}

.md-toc-item {
    color: #A8C2DC;
}

#write div.md-toc-tooltip {
    background-color: #363B40;
}

.dropdown-menu .btn:hover,
.dropdown-menu .btn:focus,
.md-toc .btn:hover,
.md-toc .btn:focus {
    color: white;
    background: black;
}

#toc-dropmenu {
    background: rgba(50, 54, 59, 0.93);
    border: 1px solid rgba(253, 253, 253, 0.15);
}

#toc-dropmenu .divider {
    background-color: #9b9b9b;
}

.outline-expander:before {
    top: 2px;
}

#typora-sidebar {
    box-shadow: none;
    border-right: 1px dashed;
    border-right: none;
}

.sidebar-tabs {
    border-bottom:0;
}

#typora-sidebar:hover .outline-title-wrapper {
    border-left: 1px dashed;
}

.outline-title-wrapper .btn {
    color: inherit;
}

.outline-item:hover {
    border-color: #363B40;
    background-color: #363B40;
    color: white;
}

h1.md-focus .md-attr,
h2.md-focus .md-attr,
h3.md-focus .md-attr,
h4.md-focus .md-attr,
h5.md-focus .md-attr,
h6.md-focus .md-attr,
.md-header-span .md-attr {
    color: #8C8E92;
    display: inline;
}

.md-comment {
    color: #5a95e3;
    opacity: 1;
}

.md-inline-math svg {
    color: #b8bfc6;
}

#math-inline-preview .md-arrow:after {
    background: black;
}

.modal-content {
    background: var(--bg-color);
    border: 0;
}

.modal-title {
    font-size: 1.5em;
}

.modal-content input {
    background-color: rgba(26, 21, 21, 0.51);
    color: white;
}

.modal-content .input-group-addon {
    color: white;
}

.modal-backdrop {
    background-color: rgba(174, 174, 174, 0.7);
}

.modal-content .btn-primary {
    border-color: var(--primary-color);
}

.md-table-resize-popover {
    background-color: #333;
}

.form-inline .input-group .input-group-addon {
    color: white;
}

#md-searchpanel {
    border-bottom: 1px dashed grey;
}

/** UI for electron */

.context-menu,
#spell-check-panel,
#footer-word-count-info {
    background-color: #42464A;
}

.context-menu.dropdown-menu .divider,
.dropdown-menu .divider {
    background-color: #777777;
}

footer {
    color: inherit;
}

@media (max-width: 1000px) {
    footer {
        border-top: none;
    }
    footer:hover {
        color: inherit;
    }
}

#file-info-file-path .file-info-field-value:hover {
    background-color: #555;
    color: #dedede;
}

.megamenu-content,
.megamenu-opened header {
    background: var(--bg-color);
}

.megamenu-menu-panel h2,
.megamenu-menu-panel h1,
.long-btn {
    color: inherit;
}

.megamenu-menu-panel input[type='text'] {
    background: inherit;
    border: 0;
    border-bottom: 1px solid;
}

#recent-file-panel-action-btn {
    background: inherit;
    border: 1px grey solid;
}

.megamenu-menu-panel .dropdown-menu > li > a {
    color: inherit;
    background-color: #2F353A;
    text-decoration: none;
}

.megamenu-menu-panel table td:nth-child(1) {
    color: inherit;
    font-weight: bold;
}

.megamenu-menu-panel tbody tr:hover td:nth-child(1) {
    color: white;
}

.modal-footer .btn-default, 
.modal-footer .btn-primary,
.modal-footer .btn-default:not(:hover) {
    border: 1px solid;
    border-color: transparent;
}

.btn-default:hover, .btn-default:focus, .btn-default.focus, .btn-default:active, .btn-default.active, .open > .dropdown-toggle.btn-default {
    color: white;
    border: 1px solid #ddd;
    background-color: inherit;
}

.modal-header {
    border-bottom: 0;
}

.modal-footer {
    border-top: 0;
}

#recent-file-panel tbody tr:nth-child(2n-1) {
    background-color: transparent !important;
}

.megamenu-menu-panel tbody tr:hover td:nth-child(2) {
    color: inherit;
}

.megamenu-menu-panel .btn {
    border: 1px solid #eee;
    background: transparent;
}

.mouse-hover .toolbar-icon.btn:hover,
#w-full.mouse-hover,
#w-pin.mouse-hover {
    background-color: inherit;
}

.typora-node::-webkit-scrollbar {
    width: 5px;
}

.typora-node::-webkit-scrollbar-thumb:vertical {
    background: rgba(250, 250, 250, 0.3);
}

.typora-node::-webkit-scrollbar-thumb:vertical:active {
    background: rgba(250, 250, 250, 0.5);
}

#w-unpin {
    background-color: #4182c4;
}

#top-titlebar, #top-titlebar * {
    color: var(--item-hover-text-color);
}

.typora-sourceview-on #toggle-sourceview-btn,
#footer-word-count:hover,
.ty-show-word-count #footer-word-count {
    background: #333333;
}

#toggle-sourceview-btn:hover {
    color: #eee;
    background: #333333;
}

/** focus mode */
.on-focus-mode .md-end-block:not(.md-focus):not(.md-focus-container) * {
    color: #686868 !important;
}

.on-focus-mode .md-end-block:not(.md-focus) img,
.on-focus-mode .md-task-list-item:not(.md-focus-container)>input {
    opacity: #686868 !important;
}

.on-focus-mode li[cid]:not(.md-focus-container){
    color: #686868;
}

.on-focus-mode .md-fences.md-focus .CodeMirror-code>*:not(.CodeMirror-activeline) *,
.on-focus-mode .CodeMirror.cm-s-inner:not(.CodeMirror-focused) * {
    color: #686868 !important;
}

.on-focus-mode .md-focus,
.on-focus-mode .md-focus-container {
    color: #fff;
}

.on-focus-mode #typora-source .CodeMirror-code>*:not(.CodeMirror-activeline) * {
    color: #686868 !important;
}


/*diagrams*/
#write .md-focus .md-diagram-panel {
    border: 1px solid #ddd;
    margin-left: -1px;
    width: calc(100% + 2px);
}

/*diagrams*/
#write .md-focus.md-fences-with-lineno .md-diagram-panel {
    margin-left: auto;
}

.md-diagram-panel-error {
    color: #f1908e;
}

.active-tab-files #info-panel-tab-file,
.active-tab-files #info-panel-tab-file:hover,
.active-tab-outline #info-panel-tab-outline,
.active-tab-outline #info-panel-tab-outline:hover {
    color: #eee;
}

.sidebar-footer-item:hover,
.footer-item:hover {
    background: inherit;
    color: white;
}

.ty-side-sort-btn.active,
.ty-side-sort-btn:hover,
.selected-folder-menu-item a:after {
    color: white;
}

#sidebar-files-menu {
    border:solid 1px;
    box-shadow: 4px 4px 20px rgba(0, 0, 0, 0.79);
    background-color: var(--bg-color);
}

.file-list-item {
    border-bottom:none;
}

.file-list-item-summary {
    opacity: 1;
}

.file-list-item.active:first-child {
    border-top: none;
}

.file-node-background {
    height: 32px;
}

.file-library-node.active>.file-node-content,
.file-list-item.active {
    color: white;
    color: var(--active-file-text-color);
}

.file-library-node.active>.file-node-background{
    background-color: rgb(34, 34, 34);
    background-color: var(--active-file-bg-color);
}
.file-list-item.active {
    background-color: rgb(34, 34, 34);
    background-color: var(--active-file-bg-color);
}

#ty-tooltip {
    background-color: black;
    color: #eee;
}

.md-task-list-item>input {
    margin-left: -1.3em;
    margin-top: 0.3rem;
    -webkit-appearance: none;
}

.md-mathjax-midline {
    background-color: #57616b;
    border-bottom: none;
}

footer.ty-footer {
    border-color: #656565;
}

.ty-preferences .btn-default {
    background: transparent;
}
.ty-preferences .btn-default:hover {
    background: #57616b;
}

.ty-preferences select {
    border: 1px solid #989698;
    height: 21px;
}

.ty-preferences .nav-group-item.active {
    background: var(--item-hover-bg-color);
}

.ty-preferences input[type="search"] {
    border-color: #333;
    background: #333;
    line-height: 22px;
    border-radius: 6px;
    color: white;
}

.ty-preferences input[type="search"]:focus {
    box-shadow: none;
}

[data-is-directory="true"] .file-node-content {
    margin-bottom: 0;
}

.file-node-title {
    line-height: 22px;
}

.html-for-mac .file-node-open-state, .html-for-mac .file-node-icon {
    line-height: 26px;
}

::-webkit-scrollbar-thumb {
    background: rgba(230, 230, 230, 0.30);
}

::-webkit-scrollbar-thumb:active {
    background: rgba(230, 230, 230, 0.50);
}

#typora-sidebar:hover div.sidebar-content-content::-webkit-scrollbar-thumb:horizontal {
    background: rgba(230, 230, 230, 0.30);
}

.nav-group-item:active {
    background-color: #474d54;
}

.md-search-hit {
    background: rgba(199, 140, 60, 0.81);
    color: #eee;
}

.md-search-hit * {
    color: #eee;
}

#md-searchpanel input {
    color: white;
}

 .typora-export li, .typora-export p, .typora-export,  .footnote-line {white-space: normal;} 
</style>
</head>
<body class='typora-export os-windows' >
<div  id='write'  class = 'is-node first-line-indent'><p><span>A Shape-Independent-Method for Pedestrian Detection with Far-Infrared-Images</span>
<span>Yajun Fang+, Keiichi Yamada+∗, Yoshiki Ninomiya*, Berthold Horn+, Ichiro Masaki+ +Intelligent Transportation Research Center, Microsystems Technology Labs Massachusetts Institute of Technology, Cambridge, MA 02139, USA ∗Toyota Central R &amp;D Labs, Inc. Japan yjfang@mit.edu, yamakei@mit.edu, nino@milab.tytlabs.co.jp, bkph@ai.mit.edu, masaki@mit.edu</span>
<span>Abstract</span>
<span>Night-time driving is more dangerous than day-time driving — particularly for older drivers. Three to four times as many deaths occur at night than in the day time [1]. To improve safety of night driving, automatic pedestrian detection based on infrared images has drawn increasing attention because pedestrians tend to stand out more against the background in infrared images than they do in visible light images. Nevertheless, pedestrian detection is by no means trivial in infrared images — many of the known diﬃculties carry over from visible light images, such as image variability occasioned by pedestrians being in diﬀerent poses. Typically, several diﬀerent pedestrian templates have to be used in order to deal with a range of poses. Furthermore, pedestrian detection is diﬃcult because of poor infrared image quality (low resolution, low contrast, few distinguishable feature points, little texture information, etc.) and misleading signals To address these problems, this paper introduces a shape-independent pedestrian detection method. Our segmentation algorithm ﬁrst estimates pedestrians’ horizontal locations through “Projection-based” horizontal segmentation, and then determines pedestrians’ vertical locations through “Brightness/Bodyline-based” vertical segmentation. Our classiﬁcation method deﬁnes multi-dimensional histogram-, inertial-, and contrast-based classiﬁcation features, which are shape-independent, complementary to one another, and capture the statistical similarities of image patches containing pedestrians with diﬀerent poses. Thus, our pedestrian detection system needs only one pedestrian template — corresponding to a generic walking pose — and avoids brute-force search for pedestrians throughout whole images, which typically involves brightness-similarity-comparisons between candidate image patches and a multiplicity of pedestrian templates. Our pedestrian detection system is not based on tracking, nor does it depend on camera calibration to determine the relationship between an object’s height and its vertical image locations. Thus, it is less restricted in applicability. Even if much work is still needed to bridge the gap between present pedestrian detection performance and the high reliability required for real-world applications, our pedestrian detection system is straightforward and provides encouraging results in improving speed, reliability, and simplicity.</span>
<span>I. Introduction Automaticdetection of pedestrians at nighthas attracted more and more attention. Eightypercentof police reports [8] cited driver errors as the primary cause of vehicle crashes. Because depth perception, color recognition, and peripheral vision are all impaired after sundown, 3–4 times as many deaths occur during night-time driving than day-time driving. Also, people’s visual capabilities deteriorate substantially as they age, as is shown in ﬁgure 1, which compares the visual ability of a driver of age 60 with those of a driver of age 20. A 50-year-old driver needs twice as much light to see as does a 30-year-old [1].</span>
<span>Fig. 1. Vision Degradation for Senior People. (Image Source: MIT Age Lab.)</span>
<span>To enhance safety, current night vision systems use infrared-cameras to provide visual aids projected on a heads-up display. In the long run, however, automatic pedestrian detection and warning is envisioned so that drivers can respond promptly without being distracted by added gadgetry. Compared to the vast research on pedestrian detection based on visible light images </span>[<span>2</span>][3][<span>5</span>][14][<span>16</span>][19]<span> as summarized in </span>[<span>11</span>][12]<span>[18], work on infrared-based pedestrian detection research </span>[<span>11</span>][12]<span>[18] has just started. In an earlier paper [6], we systematically compared diﬀerent properties of visible and infrared images and noted several unique features of infrared-based pedestrian detection. In this paper, we further investigate the statistical properties of these features and introduce a novel “shape-independent” pedestrian detection scheme including automatic pedestrian image size estimation and multi-dimensional shape-independent classiﬁcation. In</span>
<span>this section, we ﬁrst discuss how we evaluate detection performance, then review previous work and analyze challenges associated with automatic pedestrian detection using infrared images. Finally we will discuss the advantages of our design as well as diﬀerences from conventional methods. A. Performance Index Pedestrian detection [5] [14] includes two phases: “segmentation” locates multiple regions of interest (ROIs) from infraredimages, and“classiﬁcation”identiﬁespedestriansfromtheROIs. Inthispaper, weevaluatebothsegmentationand classiﬁcation performance. For segmentation, we deﬁne two new performance indices: segmentation side-accuracy and segmentation side-eﬃciency as shown in ﬁgure 2(a). “Segmentation side-accuracy” is deﬁned as the square root of ratio of the detected pedestrian region area Soverlap over the entire pedestrian area Spedestrian, which indicates how much of the pedestrian regions are captured. If, for example, the segmentation side-accuracy is 50%, then the width and height of the detected region might be only half of the actual pedestrian’s width and height. “Segmentation side-eﬃciency” is deﬁned as the square root of the ratio of the detected pedestrian area Soverlap over the entire ROI area SROI, which indicates how eﬃcient the selection of the ROI regions is. If, for example, the segmentation side-eﬃciency is 50%, then the width and height of the detected pedestrian region might be only half of the actual ROI’s width and height. Both performance measures lie in the range [0, 1]. The best segmentation performance is achieved when both measures are 1, which means that ROIs and actual pedestrian regions overlap completely. High segmentation accuracy with low eﬃciency indicates that, while most pedestrian regions are detected, this is at the cost of a large unnecessary ROI area. Conversely, low segmentation accuracy with high eﬃciency indicates that the ROIs capture only a small portion of the pedestrians, though most ROI regions are within pedestrian regions.</span>
<span>(a) (b)</span>
<span>Fig. 2. Segmentation/Classiﬁcation Performance Index Deﬁnition. (a): Segmentation Accuracy/Eﬃciency deﬁnition. (b): ROC boundary/curve deﬁnition for multi-dimensional-feature based classiﬁcation: false alarm rate (X axis) vs. detection rate (Y axis). Diﬀerent points correspond to multi-dimensional-feature-based classiﬁcation results using diﬀerent multi-dimensional thresholds. Solid curve is ROC boundary, the upper/left boundary of all classiﬁcation performance points. Dashed and dotted curves are ROC curves for 1D-feature-based classiﬁcation.</span>
<span>Toevaluateclassiﬁcationperformanceformulti-dimensional-feature-basedclassiﬁcation, weusediﬀerentmulti-dimensional thresholds, andplotcorrespondingfalse-alarm/detectionratesaspointsina2Dperformancespace(X axis: false alarm rate/Y axis: detection rate) as shown in ﬁgure 2(b). Classiﬁcation performance improves when a performance point moves toward the upper and left direction. Obviously the best performance is at the upper/left corner — with 100% detection rate and 0% false-alarm rate. However, if one point is to the upper and right of another point, we cannot easily compare their performance. The upper/left boundary of classiﬁcation performance points, as shown in solid curves in ﬁgure 2(b), can be used to demonstrate the classiﬁcation ability of an algorithm, and is called the ROC (Receiver Operating Characteristics) boundary in this paper. The ROC boundary for 1D based classiﬁcation degrades to the conventional ROC curve as shown by the dotted and dashed curves in ﬁgure 2(b). The ideal ROC curve/boundary is approximately a vertically ﬂipped “L” shape as shown in ﬁgure 2(b). All ROC curves/boundaries include two points, (0,0) and (100%,100%), which can be achieved by rejecting all or accepting all. In this paper, detection/false-alarm rates on the ROC curves are shown for ROIs (rather than image frames). We also calculate frame detection/false-alarm rate for performance comparison with other published results. To calculate the number of detected frames, we count frames in which all pedestrians are detected, and empty frames (with no pedestrian) in which there is no false alarm. However we do not plot an ROC curve for frame detection/false-alarm rates since it also depends on segmentation performance and consequently does not necessarily pass through the (100%,100%) detection/false-alarm rate point, which is diﬀerent from typical ROC curves.</span>
<span>2</span>
<span>B. Challenges and Reviews for Pedestrian Detection with Infrared Images Pedestrian detection using infrared images has its own advantages as well as disadvantages </span>[<span>6</span>][11][<span>12</span>][18]<span> when compared with detection using visible light images. In general, pedestrians emit more heat than static background objects, such as trees, roads, etc. In far-infrared images, pedestrian brightness tends to be less impacted by lighting, color, texture, and shadow information than it is in visible light imagery, and is generally also somewhat brighter than the background. However, infrared image intensities depend not only on object temperature but also on object surface properties (emissivity, reﬂectivity, and transmissivity), surface orientation, wavelength, etc. Infrared images have their own characteristics that lead to detection diﬃculties. First, non-pedestrian objects, such as, animals, vehicles, transformers, electric boxes, roads, construction areas, light poles, etc., produce additional “bright areas” in infrared images, especially in summer. These additional sources of image clutter make it impossible to reliably detect pedestrians based only on their brightness. Secondly, the image intensities of the same objects are not uniform. Pedestrian orientation, clothes, accessories (such as backpacks), etc., all have an impact on observed image intensity patterns. Body-trunk areas are generally darker than head and hand areas, especially when pedestrians wear heavy coats or carry backpacks. The upper parts of light poles appear brighter than the lower parts because of contrast phenomena in typical far-infrared cameras. Non-homogeneous optical properties add to detection diﬃculties. Thirdly, most infrared image intensities have a smaller intensity range than do comparable visible images. This leads to low image quality: blur, poor resolution and clarity, low foreground/background contrast, fewer feature points and less texture information, etc. Thus, current infrared-based pedestrian detection research is still limited </span>[<span>6</span>][7][<span>11</span>][12][<span>16</span>][18]<span>. Both segmentation accuracy and classiﬁcation reliability of early night vision research needs to be signiﬁcantly improved for it to be used practically </span>[<span>18</span>][12]<span>. For example in winter, to have a false-alarm rate around 2.63% [18], the detection rate has to be limited to only 35%. In summer, to have 75% to 90% detection rate, the false-alarm rate has to be raised to 100% [12] as shown in ﬁgure 3(a). Below we will discuss inherent diﬃculties in two phases and then review related current work.</span>
<span>(a) (b) (c)</span>
<span>Fig. 3. Algorithm and Performance Comparison for Diﬀerent Pedestrian Detection Methods. (a): Detection Performance Comparison. (b): Detection Algorithm Comparison. (c): Default Pedestrian Template for MIT Shape-Independent Method. Image size: 58×21.</span>
<span>B.1 Challenges and Reviews for ROI Segmentation It is diﬃcult to segment pedestrians in real-world video images captured by moving-cameras mounted on vehicles. Pedestrians have a variety of poses, sizes and appearances, and the background is changing rapidly as the cameras move through the environment. Many conventional fast segmentation algorithms have been developed for stationary cameras, such as “Background Subtraction”[15], “Motion Calculation,” and “Tracking.” These methods assume similar background or feature points, and need initialization time. Thus it is expected that pedestrian detection for intelligent vehicles can rely only on single static image instead of multiple-image-based (motion-based) algorithms. Conventionally, segmentation based on depth information is more straightforward than other methods and multi-scale brute force searching can be avoided. However binocular-infrared-camera-setup is not widely used in most night vision research, except by Tsuji (2001) [16]. There might be reliability concern because of the properties of infrared images discussed above and nodding movement of cameras on vehicles [6]. If detailed pedestrian contours can be extracted, pedestrians can be identiﬁed by using “Contour-based Shape Model” </span>[<span>2</span>][3]<span>[5], such as, pedestrian shapes hierarchy[5] or</span>
<span>3</span>
<span>human walking model[3]. Besides, “Human Component Features” </span>[<span>4</span>][5][<span>10</span>][13]<span>[17], such as skin hue, eyes, faces, etc., also help when segmenting pedestrians in visible images. The above well known fast segmentation features are non-applicable to far-infrared images because of their unique properties. It is also hard to segment pedestrians by grouping bright spots belonging to pedestrians based only on their pixel intensities. Using one ﬁxed brightness threshold, for example, will lead to several separated bright spots at both pedestrian regions and other noise resources, with results highly sensitive to the choice of brightness thresholds. If introducing “Template-Shape-based” multi-scale brute-force searching as some night vision algorithms do (as shown in Figure 3(b)), segmentation ROI outputs are all candidate pedestrian patches of diﬀerent sizes and aspect ratios, at multiple initial locations. The total number of ROIs for completely blind multi-scale brute force searching is as follows: nROI =Pnscale i=i nicenter−pos ∝ nrow ∗ncolumn ∗nscale (1) where nscale is the number of scales in estimating pedestrian sizes, nicenter−pos is proportional to the image size (n row∗ncolumn ), which is the number of initial ROI center positions that must be tried when testing at diﬀerent scales. The large search space for blind searching is a serious limitation in Diﬀerent segmentation algorithms take advantage of diﬀerent features to decrease nROI and to expedite the searching process. To decrease ncenter−pos, [18] searches bright and round regions as potential pedestrian heads in infrared images. [11] searches hot symmetrical ROIs with speciﬁc size and aspect ratio based on the “Symmetry Property” of pedestrians and their brightness[9]. To decrease nscale, [18] and [11] assume ﬂat roads so that pedestrians’ distance can be estimated based on pedestrians’ vertical positions in images. [18] ﬁrst detects road surface boundaries in order to estimate pedestrians’ sizes and height and remove impossible pedestrian size/position combinations. [11] calibrates infrared cameras to build correspondences between image lines and distances in the 3D world for pedestrian size estimation. [12] does not make any assumptions and searches only three pedestrian sizes in a multi-scale brute force approach. The segmentation accuracy is limited compared with </span>[<span>11</span>][18]<span>. For real-world applications, segmentation algorithms need to further improve speed and accuracy and make fewer assumptions on the driving environment. B.2 Challenges and Review for Classiﬁcation In far-infrared images, pedestrians yield widely varying image patterns because of the imaging complexity mentioned before and variations in pedestrian poses. When presented with multiple candidate image regions, diﬀerentiating pedestriansfrom non-pedestrian regions is diﬃcult. Typicallythe decision is made based on the similaritybetweenROIregions and multiple pedestrian templates with various poses and appearances. Similarity can be computed either directly or indirectly. Typical direct methods compare image intensity pixel-by-pixel and compute the “Image-Intensity-Diﬀerence” between two patches, i.e., the Frobenius norm of image pixel intensity diﬀerences. The classiﬁcation methods heavily depend on shape matching and as a result are sensitive to segmentation errors and variations in pedestrian poses. [12] deﬁnes a template probabilistic model to encodes the shape information of pedestrians and the variations that the shape can undergo by describing the possibility of foreground and background at each pixel based on training data. [11] identiﬁes pedestrians through matching candidates with a simple model that encodes morphological characteristics of a pedestrian. The shape-dependent ﬁlter removes candidates that do not present a human shape or are not as hot as expected for a pedestrian. For indirect similarity-comparison, shape-dependent pedestrian-intensity-arrays are used to train classiﬁers to capture the similarity between pedestrian training samples and ROIs, for example, support Vector Machine [14] [18], Neural Network </span>[<span>13</span>][19]<span>, Posteriori Detection (including Polynomial Classiﬁers, Multi-Layer Perceptrons, and Radial-basis Functions), etc., (as shown in ﬁgure 3(b)). [18] proposed SVM (support vector machine) classiﬁers for three types of pedestrians for infrared images. These brightness-similarity-comparison based classiﬁcation methods are shape-dependent, and might miss pedestrians with unusual poses even if multiple pedestrian-pose-templates or training samples are used. Furthermore, complicated machine-learning methods require signiﬁcant computational resources. In summary, speed, reliability, and performance robustness to pose-changes and segmentation errors are serious concerns for real-world night vision systems. C. The Methodology and Principle for “Shape-Independent” Pedestrian Detection Because of the above mentioned diﬃculties involved in shape dependent and/or brute-force searching based methods, the performance of present pedestrian detection systems is limited as shown in ﬁgure 3(a). In this paper, we introduce a “Shape-Independent”automaticpedestriandetectionmethodwithstraightforwardimplementation. Figure3(b)presents the major diﬀerences between our “shape-independent” methods and conventional “shape-dependent” methods. The algorithm can automatically estimate the horizontal location of candidate pedestrian regions to avoid brute-force multiscale searching. Our novel classiﬁcation feature vectors can characterize the statistical similarity of multiple pedestrian regions with diﬀerent poses, and can also capture the statistical diﬀerences between pedestrian and non-pedestrians regions in infrared images. Thus, our multi-dimensional classiﬁcation needs only one generic pedestrian template as</span>
<span>4</span>
<span>shown in ﬁgure 3(c) with size 58×21 (details in section III). The method is based on the unique statistical properties of far-infrared images that we discovered through investigating the diﬀerences between visible and infrared images [6]. Our method has the following properties. First, it focuses on improving combined segmentation/classiﬁcation systems and balances the complexity and performance of two subsystems instead of maximizing one process while sacriﬁcing the other. This is because accurate segmentation can ease the classiﬁcation task and robust classiﬁcation can tolerate segmentation errors. Secondly, our segmentation procedure is robust to threshold choices. Finally, our algorithm does not make constraining assumptions for background, for example that ﬂat roads; thus our results are very general. The classiﬁcation performance comparison is shown in ﬁgure 3(a). For pedestrian detection in winter, we achieve a higher detection rate when we set the false alarm rate to be similar to other available published results. For summer, we achieve a lower false alarm rate when we set the detection rate to be similar to other available published results. In the rest of the paper, we will introduce our “Automatic Pedestrian Segmentation,” and “Shape-Independent Multiple Dimensional Classiﬁcation” respectively in section II and III. Performance evaluation and future work will be discussed in section IV and section V.</span>
<span>II. Automatic Pedestrian Segmentation As mentioned in I-B.1, conventional “Template-Shape-based” segmentation involves searching with computational load Ø(n2). We invented a new “horizontal-ﬁrst, vertical-second” segmentation scheme involving only 1D searching in vertical direction with computational load Ø(n). The method ﬁrst automatically estimates the horizontal locations of candidate pedestrian regions, and then searches for pedestrians images vertically within the corresponding image stripes (from top to bottom in the images) at the estimated horizontal positions. Thus search space and computational load are reduced signiﬁcantly. In this section, we will respectively introduce our “horizontal segmentation” algorithm based on “bright-pixel-vertical-projection curves,” and “vertical segmentation” based on “brightness/bodylines.” A. Horizontal Segmentation Here below we will ﬁrst deﬁne the “bright-pixel-vertical-projection curve,” then explain how and why we can use this concept to estimate pedestrians’ horizontal locations. A.1 Bright-pixel-vertical-projection Curves For an infrared image, we deﬁne its bright-pixel-vertical-projection curves as the number of bright pixels in image columns versus their corresponding horizontal positions. To count “Bright Pixels,” the intensity threshold is adaptively deﬁned as follows:</span>
<span>Bright Pixel Threshold =max(Image Intensity) - Intensity Margin (2)</span>
<span>where the variable “Intensity Margin” is a ﬁxed constant for diﬀerent video sequences. Typically the bright-pixelvertical-projection curves can be divided into several “bumps” or “waves” with rising left curves and falling right curves, as well as ﬂat regions with zero height whose corresponding image stripes have no bright pixel as shown in ﬁgure 4(b). Each pedestrian image will be captured in one image stripe corresponding to one such wave. In most cases the width of the pedestrian-image-region is equal to the width of the corresponding wave as shown in ﬁgure 4(a)(b). The features of the deﬁned curve is robust to the choices of brightness threshold and problems mentioned in section IB. Figure 4(a) shows the variation of projection curves corresponding to two diﬀerent brightness thresholds. Generally, the height and shape of the “waves” in the curves will change. However the horizontal locations and width of waves corresponding to pedestrians will not change signiﬁcantly. This is because of the following two reasons: First, for typical real-world far-infrared images, the image stripe containing one pedestrian is narrow and the number of bright background pixels in each column can be treated as more or less constant unless there happens to be some light poles, for example, which may tend to also appear narrow and bright, at least in summer. Later we will discuss how pedestrians can be detected in this special case. Secondly, image columns passing through pedestrian regions tend to encounter more bright pixels than neighbor columns passing background regions. Both of these features are independent of the choices of brightness thresholds. A.2 Projection-based Horizontal Segmentation Algorithm Based on the above properties of the bright-pixel-vertical-projection curves, we can segment an image horizontally into several image stripes (as shown in the bottom row of ﬁgure 4(b)), some of which contain individual pedestrians and roughly determine candidate pedestrians’ image width. The procedure is as follows: 1. Adaptively choose an brightness threshold using equation (2). Record the number of bright pixels in each column in the bright-pixel-vertical-projection curves. We select a large constant “Intensity Margin” in equation (2) that</span>
<span>5</span>
<span>(a) (b)</span>
<span>(c) (d)</span>
<span>Fig. 4. The Feature of Bright-Pixel-Vertical-Projection Curves for Infrared Images and Brightness-based Vertical Segmentation Results. For (a)(c): Winter results. For (b)(d): Summer results. (a): Top row: original infrared image in winter. Center row and Bottom row: Bright-Pixel-Vertical-Projection curves when using two diﬀerent thresholds. (b): Top row: original infrared image in summer. Center row: Bright-Pixel-Vertical-Projection Curve. Bottom row: Horizontally segmented image stripes based on projection curve. Note that Several separated stripes shown in the center row seem to be connected. For (c)(d): Brightness-based vertical segmentation results. For all projection curves: X axis: Image column position. Y axis: Number of bright pixels in each column.</span>
<span>makes the brightness threshold adaptively small to ensure that the image columns containing pedestrians will have non-zero projection in the bright-pixel-vertical-projection curves. 2. Automatically search for the starting points of all rising curves (wave-start-points) and the ending points of all falling curves (wave-end-points). 3. Separate the bright-pixel-vertical-projection curves into several waves by pairing wave-start-points and wave-endpoints, and ignoring ﬂat regions of zero height. 4. Record image stripes corresponding to these “waves.” Because of background brightness “noises” in summer, projection curves for winter and summer images, as shown in ﬁgure 4(a)(b), have the following diﬀerent properties. First, in winter, waves corresponding to pedestrians usually have higher peaks than background waves, unlike summer where background “noises” may produce high wave peaks in projection curves as shown in ﬁgure 4(b). Secondly, under complicated urban driving scenario in summer, as shown in ﬁgure 4(b), pedestrians and background brightness “noises” may be spatially proximate and their projection thus may merge into one wave, which is the case for the second pedestrian from the left in ﬁgure 4(d). For winter images (example: sequence 1 shown in ﬁgure 17(a1)) and summer sequences in suburban area (example: sequence 2 in ﬁgure 17(b1)) with sparse foreground objects, pedestrian regions are less likely to be grouped with other “hot” foreground regions. In spite of the diﬀerences that might make image stripes wider than the actual pedestrian image width in some cases, pedestrians will be fully captured in individual horizontally separated stripes. So far, we have presented a novel projection-based pedestrian pre-segmentation algorithm that horizontally separates infrared images into several image stripes that may contain pedestrians. In the next section, we will introduce how to</span>
<span>6</span>
<span>search pedestrians’ vertical location in segmented image stripes. B. Vertical Segmentation within Horizontally Segmented Image Stripes Here we will introduce two vertical segmentation algorithms. The ﬁrst is a “Brightness-based” method (section II-B.1) that works best in winter and suburban situations where most segmented image stripes for pedestrians reﬂect the true width of pedestrian-image-regions. The second is a “Bodyline-based” method (section II-B.2) for more complicated scenarios where the image stripes containing pedestrians might be wider than the pedestrian images true width. These two methods provide complementary results that work best in diﬀerent scenarios, and the results from both methods are sent to classiﬁcation step to further improve reliability and accuracy. B.1 Vertical Segmentation based on Brightness After obtaining horizontally segmented image stripes from section II-A.2, the vertical positions of candidate pedestrian regions can be estimated by the highest and the lowest vertical locations of bright pixels within these stripes. This method is applicable when the estimate of the pedestrian region width is reasonably accurate. In this case, most brightness-based vertical segmentation results for both winter and summer data turn out correctly as shown in ﬁgure 4(c)(d). Our classiﬁcation algorithm has the ability to tolerate segmentation errors for pedestrian ROIs, such as the inclusion of extra background regions or conversely missed portion, as shown in the ﬁrst and the fourth pedestrians from the left in ﬁgure 4(c). Non-pedestrian ROIs have bright pixels at the boundaries, which facilitates the inertialbased classiﬁcation algorithm to be described later in section III-B. When segmentation stripes are much wider than the actual pedestrian image size, ROIs may be much larger than the true width, as occurs for the third pedestrian from the right in ﬁgure 4(d). A “Bodyline-based” vertical segmentation algorithm (explained below) is proposed to improve segmentation performance in such more diﬃcult situations. B.2 Vertical Segmentation based on Bodyline In this method, we reﬁne the pedestrian width estimation by detecting pedestrian regions’ left and right boundary pointswithinsegmentedimagestripes. Thuswecanfurthersearchforpedestrians’verticalpositionsbasedonageometric pedestrian-size-model as described next. For each row of image stripes, we deﬁne the portion of image rows within pedestrian regions as the pedestrianbodyline, and deﬁne prominent feature points where image rows meet pedestrian boundaries as pedestrian-bodylineterminals. Figure 5(a) presents one “bodyline” example in the waist area of a pedestrian image. Below we will describe in detail how to detect bodyline, and how to vertically segment pedestrians within image stripes. Step 1 Pedestrian Horizontal Bodyline Detection. Because of infrared image features, in each row within segmented image stripes, the left pedestrian-bodylineterminals are the points where image intensities change from darkness to brightness most rapidly. Similarly, at the right pedestrian-bodyline-terminals, image intensities change from brightness to darkness most rapidly. To obtain pedestrian-bodyline-terminals, we calculate intensity variation along the horizontal direction based on the modiﬁed Sobel method as below:</span>
<span>∆I(x,y) = [I(x+1,y +1)−I(x−1,y +1)+2I(x+1,y)−2I(x−1,y)+I(x+1,y−1)−I(x−1,y−1)]/6 (3) where (x,y) are pixel coordinates, I(x,y) is image intensity, and ∆I(x,y) is pixel-horizontal-spacing. We ﬁrst calculate “pixel-horizontal-spacing” for all pixels in each row within horizontal segmentation stripes. Then we search in the left half portion of the row for a point with the largest “pixel-horizontal-spacing” as the candidate for the left bodyline terminal points. We skip the row where “pixel-horizontal-spacing” for all pixels is smaller or equal to zero. Similarly, we determine the right bodyline terminal with the most negative “pixel-horizontal-spacing” in the right half portion of the row. Thus we obtain two outmost boundaries and a bodyline for candidate pedestrians in each row within horizontal segmentation stripes. For segmented image stripes shown in ﬁgure 4(b), ﬁgure 5(b) preserves all pixels within detected candidate bodylines, in which pedestrians stand out and the background pixels surrounding the pedestrian regions has been removed. It may happen that some boundary points belong to other “hot objects” next to the pedestrians, and we might not obtain a clear bodyline at every row of pedestrian regions. However, as long as we can obtain one bodyline, in the next step we can still estimate the candidate pedestrian’s image size based on the bodyline length. Step 2 Pedestrian Location Estimation based on Pedestrian-Bodyline Matching In ﬁgure 5(a), we propose a geometric pedestrian-size-model that deﬁnes one pedestrian’s size and location based on the location and length of a waist-bodyline. The reason we use waist-bodylines is because the contrast between</span>
<span>7</span>
<span>human hip areas and their local background neighborhoods tend to be robust to the poses of walking pedestrians. Horizontal waist-bodylines are more likely to be detected and are not easily missed under a variety of conditions. Using the model, we can deﬁne multiple candidate pedestrian regions by assuming each detected bodyline to be the waist-bodyline of an pedestrian. Figure 5(c) provides an example of bodyline-based pedestrian location estimation. A few estimated candidate pedestrian regions are marked. Step 3 Histogram-based Bodyline/Pedestrian Searching Among multiple candidate regions deﬁned previously within a vertical image stripe, there is at most one actual pedestrian image region. Choosing one candidate pedestrian region is essentially a classiﬁcation problem. We ﬁrst use one histogram-based classiﬁcation feature to search for the best candidate within each image stripe. After obtaining one candidate for each image stripe, we further determine whether it is a actual pedestrian image using multi-dimensional classiﬁcation features explained in the next section. Details of the histogram-based feature and other classiﬁcation features will be explained in section III. It is worth mentioning that we do not need to use a threshold in the searching process since we choose ROIs that are closest to our default pedestrian template (ﬁgure 3(c)) in histogram feature space.</span>
<span>(a) (b) (c) (d)</span>
<span>Fig. 5. Pedestrian segmentation based on two diﬀerent methods. (a): Bodyline based geometric pedestrian-size-model. (b): Bodyline image. (c): Candidate pedestrian region estimation based on (a) and (b). (d): Bodyline-based segmentation result.</span>
<span>For initial horizontally segmented image stripes in the bottom row of ﬁgure 4(b), the bodyline-based vertical segmentation result is shown in ﬁgure 5(d), which provide more accurate segmentation results than the “brightness-based segmentation” results shown in ﬁgure 4(d) where background noise causes segmentation errors. In sum, the ﬂowchart of automatic pedestrian segmentation starts with “projection-based” horizontal segmentation as shown in ﬁgure 3(b). Within segmented image stripes, “brightness-based” vertical segmentation assumes that pedestrian pixels are brighter than the rest of background pixels in the image stripes. The “bodyline-based” method assumes there exists clear brightness contrast between pedestrian image regions and their horizontal-neighbor-regions, and search for the left-positive/right-negative vertical-edge-pairs with high “pixel-horizontal-spacing” in order to detect potential pedestrian bodylines and estimate candidate pedestrian positions. Both methods automatically estimate pedestrians’ sizes and avoid multi-scale brute force searching. The ﬁrst method is straightforward and works reliably in suburban summer cases as well as winter cases. The second method works in complicated urban driving situations. Neither method needs to assume ﬂat roads and both can work in a general driving situation. In real-world applications, both segmentation results will be fused in classiﬁcation. Conventional segmentation involves brute force searching within an entire image and produces multiple initial ROIs as in equation (1). Instead, “bodyline-based” segmentation involves only searching among multiple bodylines within horizontally segmented image stripes and the number of produced initial ROIs is as follows:</span>
<span>nbodyline ROI ≈nimage stripe ∗nbodyline (4) where nimage stripe is the number of horizontally segmented image stripes and is usually less than 20 (even less than the number of image columns), and nbodyline is the largest number of bodylines in segmented image stripes and is much less than the number of image rows. Thus nbodyline ROI is signiﬁcantly less than in equation (1). The number of ROIs for “brightness-based” segmentation is equal to nimage stripe. In sum, our vertical segmentation produces fewer candidate ROIs.</span>
<span>III. Classification To recognize pedestrians, conventional classiﬁcation is based on brightness-similarity-comparisons between ROIs and multiple templates, which is shape-dependent and is subject to segmentation errors and pose-changes as mentioned in</span>
<span>8</span>
<span>sectionI-B.2. Forrobustnessandreliability, weproposeinnovativeclassiﬁcationthatisbasedoncomparingthesimilarity between multi-dimensional shape-independent feature vectors for ROIs and for one generic pedestrian template. In this section we ﬁrst introduce histogram-, inertial-, and contrast-based classiﬁcation features individually, then we will propose our multi-dimensional classiﬁcation methods and compare the classiﬁcation ability of our deﬁned shapeindependent features with conventional shape-dependent features. A. Histogram-based Classiﬁcation In this section we discuss the brightness histogram similarities among pedestrian regions with various poses, sizes and appearances, and introduce the histogram-feature’s ability to separate pedestrian/non-pedestrian ROIs based on one generic pedestrian template. A.1 Statistical Similarity of Brightness Histograms for Pedestrian ROIs In section I-B, we have mentioned that pedestrian regions in infrared images are complex and not homogeneous. However, when pedestrians change poses, the intensity patterns should be consistent for similar body areas in diﬀerent infrared images. Because of similar body temperatures and similar pedestrian surface properties, this observation applies not only for the same pedestrian of diﬀerent poses, but also for diﬀerent pedestrians with diﬀerent gender, clothing, and in diﬀerent seasons. Thus, there exists the similarity among image-brightness-histogram-curves for pedestrian patches containing diﬀerent people, with diﬀerent poses, and in diﬀerent seasons. This property is demonstrated in histogram curve comparison in ﬁgure 6. Figure 6(a0) is our default pedestrian template cut from a summer sequence. Figure 6(a1) shows seven pedestrian ROIs from four winter images, in which pedestrians have diﬀerent poses and are of diﬀerent gender. Figure 6(b1) demonstrates the similarity among the brightness-histogram-curves for the seven pedestrian regions. Figure 6(c1) compares the average brightness-histogram-curves of the above seven pedestrian regions from winter images(solid line) with the histogram curve for the pedestrian template from summer images(dashed line) in ﬁgure 6(a0). We further demonstrate statistical histogram similarity for pedestrian regions through the variation of brightnesshistogram-curves from 911 rectangular pedestrian regions in seven diﬀerent driving sequences. Figure 7(a) shows the examplesofpedestrianappearancesandsizesintwosamplesequences. Wenormalizeallpedestrianpatchestoastandard size [58×21] (1218 pixels) before calculating their smoothed brightness-histogram-curves, i.e., histmROI(i), which is the number of pixels with brightness i. Figure 7(b) deﬁnes the “histogram variation curve,” i.e., the distribution of histogram variation value hm(i)−hn(i) for all brightness i. In this way, the variation of all 911 histogram curves histmROI from their average histogram histmean is presented as the collective “histogram variation curve” in ﬁgure 7(c), which resembles a Gaussian shape (of zero mean) with certain skewness. We can see that most histogram shape variation is within [-10, 10] pixels, which is only 8.2% of the largest variation (1218 pixels). The fact provides us statistical evidence that histogram curves for pedestrian regions are very similar. A.2 The Classiﬁcation Ability of Histogram Feature Figure6(b2)showsthecomparisonamongallhistogramcurvesfornon-pedestrianROIsinﬁgure6(a2), andﬁgure6(c2) shows the comparison between their average and the brightness histogram of a summer pedestrian template, as shown in ﬁgure 6(a0). The results are drawn with the same scale as ﬁgure 6(b1)(c1) with similar comparisons for pedestrian ROIs. The comparison between ﬁgure 6 (b) and (c) reveals that histogram features for pedestrian/non-pedestrian ROIs are diﬀerent in most cases. Because of the histogram similarity for pedestrian regions, as well as histogram diﬀerences between pedestrian ROIs and non-pedestrian ROIs, pedestrians can be identiﬁed through histogram-similarity-comparison between ROIs and one generic pedestrian template. Without losing generality, we choose ﬁgure 6(a0) as our generic pedestrian template. The Histogram Diﬀerence index is deﬁned as the weighted summation for the square of brightness histogram diﬀerence at each brightness i as below: Histogram Diﬀerence = αP255 i=1 weight(i)∗[histROI(i)−histtemplate(i)]2 (5) where histROI and histtemplate are histogram curves for ROIs and a template respectively, α is normalization coeﬃcient, weight(i) is weighting function that is ﬁxed for all classiﬁcation calculations. Typically segmentation errors might introduce extra dark background or bright regions, leading to higher histogram curve peaks at small/large brightness value. Weight(i) is set to be small when brightness i is very dark or bright in order to reduce the impact of segmentation errors. The expected value for pedestrian ROIs is 0. The larger the histogram diﬀerence for an ROI, the less likely is the ROI to be a pedestrian. B. Inertial-based Classiﬁcation Inertial-based classiﬁcation feature is based on the inertial similarity among pedestrian regions, and is also shapeindependent. We deﬁne inertial value for one image patch as in equation (6):</span>
<span>9</span>
<span>(a0) (a1) (a2)</span>
<span>50 100 150 200 250</span>
<span>0</span>
<span>20</span>
<span>40</span>
<span>60</span>
<span>80</span>
<span>100</span>
<span>(b1) 50 100 150 200 250 0</span>
<span>20</span>
<span>40</span>
<span>60</span>
<span>80</span>
<span>100</span>
<span>(b2)</span>
<span>50 100 150 200 250</span>
<span>0</span>
<span>5</span>
<span>10</span>
<span>15</span>
<span>20</span>
<span>(c1) 50 100 150 200 250 0</span>
<span>5</span>
<span>10</span>
<span>15</span>
<span>20</span>
<span>(c2)</span>
<span>Fig. 6. Properties of Brightness-histogram-curves for Pedestrian/non-Pedestrian ROIs. (a0): Pedestrian from summer data. Used as default template in our algorithm. (a1): Pedestrian ROIs with diﬀerent poses. (a2): Non-pedestrian ROIs. (a1)(a2) are segmentation results for winter data. For (b1)(b2): Brightness histograms for (a1)(a2). (b1): demonstrates histogram similarity among winter pedestrian ROIs. (b2): demonstrates the histogram variation among winter non-pedestrian ROIs. For (c1)(c2): Solid lines: Average brightness histogram for winter pedestrian ROIs (b1) and winter non-pedestrian ROIs (b2) respectively. Dashed lines: Histogram curve for summer pedestrian (a0). (c1): demonstrates the histogram similarity between winter pedestrians and summer pedestrian template. (c2): demonstrates the disparity between winter non-pedestrian ROIs and summer pedestrian template. For (b1)(b2)(c1)(c2): X axis: Image intensity range (0-255). Y axis: brightness histogram. image inertial = Px,y I(x,y)d(x,y)2 Px,y Itemplate(x,y)dtemplate(x,y)2 (6)where I(x,y) is the pixel brightness values for image patches after size normalization, d(x,y) is the distance from a pixel to image center as shown in ﬁgure 8(a). Image inertial value is the summation of rotation momentum with respect to the image center for all pixels while subjected to a scaling factor. The scaling factor (denominator) is the summation of rotation momentum for all pixels in our generic pedestrian template patch in ﬁgure 6(a0). Inertial values for pedestrian patches with diﬀerent poses should be close to 1. In the next two sections, we will discuss the statistical similarity between all pedestrian ROI inertial values, and demonstrate its classiﬁcation ability. B.1 The Statistical Similarity of Pedestrian ROI Inertial For the 911 pedestrian regions mentioned in section III-A.1(examples shown in ﬁgure 7(a)), their inertial values are calculated and plotted in ﬁgure 8 (b) and their distribution is plotted in ﬁgure 8(c), which resembles Rayleigh distribution. The average inertial value is 1.03, which shows that inertial values are centered around its expected value.</span>
<span>10</span>
<span>(a1) (a2)</span>
<span>(b) (c)</span>
<span>Fig. 7. For (a1)(a2): Sample pedestrian regions from 2 sequences (every five frames) to show the variation of pedestrian poses and sizes, which correspond to the 2nd and 3rd pedestrian detection examples in section IV. (b): Left: two brightness-histogram-curves with brightness i (X axis) vs. hn(i) and hm(i) (Y axis ) that are pixel numbers with brightness i from two image regions. Right: Deﬁnition for “histogram variation curve” with all possible histogram variance value hn(i)−hm(i) (X axis) vs. variation frequency (Y axis). (c): Collective “histograms variation curve” for 911 pedestrian samples (in 7 sequences), with all possible histogram variation value (X axis) vs. the distribution of histogram variation value from all pedestrian histogram curves and their mean (Y axis).</span>
<span>Around 70% of pedestrian regions have inertial values within 0.8−1.2, around 94% of inertial values vary within 0.6−1.4. Figure 8(b) and (c) demonstrate inertial similarity for pedestrian regions in infrared images. B.2 The Classiﬁcation Ability of Inertial Feature The inertial-based feature helps to remove classiﬁcation ambiguity based on the histogram feature alone. Based on our segmentation algorithm, when pedestrian/non-pedestrian ROIs have similar brightness histograms, ROIs have similar numbers of bright pixels and some bright pixels must situate around image boundaries. For typical pedestrian ROIs, most bright pixels stay in the middle of image patches and only a few pixels at heads, hands, and feet areas touch horizontal and vertical boundaries. For typical non-pedestrian ROIs, bright pixels are less centralized with more bright pixels near horizontal and vertical boundaries, leading to diﬀerent inertial values. As shown in ﬁgure 8(a) the inertial value for the right non-pedestrian patch is larger than the left pedestrian patch despite their similar histogram feature. C. Contrast-based Classiﬁcation In infrared images, there exists brightness contrast between pedestrian regions and their horizontal and vertical neighborhoods. The horizontal brightness contrast has been used in our segmentation algorithm to obtain pedestrians’ left/right boundaries. Because of the concern of robustness, the vertical contrast is not directly used to identify pedestrians in segmentation. Instead, it can be used to identify non-pedestrian as follows. We evaluate the contrast between an ROI and its vertical neighborhoods by comparing the vertical edges within these regions as shown in ﬁgure 9(a)(b). Vertical edges are deﬁned as the image pixels with “pixel-horizontal-spacing” (deﬁned in equation (3), section II-B.2) larger than a constant threshold. For a rectangular ROI, its upper/lower vertical neighborhood is a rectangular region that is right above/below the ROI with the same column width and</span>
<span>11</span>
<span>(a) hspace.2in (b) hspace.2in (c)</span>
<span>Fig. 8. (a): ROI inertial deﬁnition. (b): Collective inertial values for all 911 pedestrian samples. X axis: Pedestrian sample order. Y axis: Inertial feature value. (c): Distribution for inertial values in (b). X axis: Inertial value. Y axis: Distribution percentage.</span>
<span>half the ROI height. The Row-edge index for a rectangular region is deﬁned as the average number of vertical edge pixels for each row of the region. Rich texture leads to a large row-edge index. The row-edge indices for an ROI and its upper and lower vertical neighborhoods are respectively called ROI row-edge index, upper row-edge index, and lower row-edge index. These three variables are deﬁned as ROI contrast-feature vectors. For an ROI, the comparison between its ROI row-edge index and upper/lower row-edge index provides vertical texture contrast information between the ROI and its vertical neighborhoods. For typical infrared images from real driving scenes, image stripes containing one pedestrian are narrow. Without losing generality, we assume there is no pedestrian at the top of another pedestrian region within one segmented image stripe. For pedestrian ROIs, the vertical neighborhoods are background whose vertical edges should be limited within narrow image stripes. Speciﬁcally, their lower vertical neighborhoods contain road areas, in which we cannot ﬁnd two long vertical lines or many vertical edge pixels beneath pedestrian ROIs. There is at most one vertical line produced by lane markers within narrow image stripes because of camera perspective, thus “Lower row-edge index” for pedestrian ROIs should not be larger than 1. If this is not the case, non-pedestrian ROIs can be identiﬁed since pedestrian ROIs present vertical contrast between ROIs and their lower neighborhoods. Similarly, in most cases, the upper row-edge indices for pedestrian ROIs should be smaller than 2 since their “upper vertical neighborhoods” contain general sky, building, trees, etc., and none of them produces two (or more than two) adjacent vertical long edges within the narrow stripes of infrared-images. The exception is when pedestrians stand right in front of “hot” light poles, which makes their upper row-edge indices be close to 2. In this case, we check their ROI row-edge indices, which should be smaller than 2 for pedestrian ROIs, because some pedestrian image rows do not have any vertical edge pixel and other rows contain at most two vertical edge pixels, i.e., pedestrian-bodyline-terminals. Thus, if both the upper row-edge indices and the ROI row-edge indices are large, there is no vertical contrast for ROIs, and non-pedestrian ROIs can be identiﬁed. The selected non-pedestrian ROIs are very likely to be in the middle of light poles, which is the case for all selected non-pedestrian ROIs in ﬁgure 9(a) that correspond to poles in ﬁgure 5(d). The “ROI row-edge index” is used to remove the ambiguity between non-pedestrian ROIs containing light poles and pedestrian ROIs in front of poles. In summary, pedestrian ROIs and their vertical neighborhoods should present vertical contrast and lead to small “upper/lower row-edge indices.” Though we cannot identify pedestrian ROIs simply based on vertical contrast, a few non-pedestrian ROIs can be identiﬁed and removed when vertical contrast does not exist based on one of the two following conditions: Case I: ‘‘Lower row-edge index’’ is larger than 1. Case II: Both ‘‘upper row-edge index’’ and ‘‘ROI row-edge index’’ are close to or larger than 1.5. The process is called “contrast-based non-pedestrian ROI-removal.” Figure 9 is an example of how we identify non-pedestrians among ten ROIs in ﬁgure 5(d) based on their “vertical-neighborhood-contrast property.” Rectangular regions for ROIs, their upper/lower neighborhood regions, and the corresponding image vertical edge pixels are plotted in ﬁgure 9(a)(b). Figure 9(a) contains all selected non-pedestrian ROIs through “contrast-based non-pedestrian ROIremoval.” For each of them, the vertical-neighborhood-contrast is vague since there are two clear vertical edges in either the upper or lower vertical neighborhoods, leading to large “upper/lower row-edge indices.” In this example, they are all light pole regions in ﬁgure 5(d). For remained ROIs in ﬁgure 9(b), including all 3 pedestrian ROIs and 3 non-pedestrian ROIs, the “upper/lower row-edge indices” are small and we need histogram/inertial classiﬁcation to separate them. It is worth mentioning that we use one constant and large threshold for all sequence frames to determine vertical edges based on their “pixel-horizontal-spacing.” Usually the performance of “contrast-based non-pedestrian ROI-removal” is robust to threshold choices since the image contrast between ROIs and their neighborhoods is robust to the threshold</span>
<span>12</span>
<span>choices. In the worst case that a threshold is too large, both “ROI row-edge indices” and “upper/lower row-edge indices” for non-pedestrian ROIs are small and the non-pedestrian ROIs cannot be removed based on the two above conditions. It is acceptable since further histogram/inertial-based classiﬁcation can identify them.</span>
<span>(a) (b) (c)</span>
<span>Fig. 9. “Contrast-based non-Pedestrian ROI Removal” for ROIs in Figure 5(d). For (a)(b): ROIs and their vertical neighborhood regions on edge map, i.e., vertical-neighborhood-contrast property. (a): For detected non-pedestrian ROIs. (b): For remained ROIs. (c): Remained ROIs on the original image.</span>
<span>C.1 Statistical Distributions of Contrast-based Classiﬁcation Feature</span>
<span>(a) (b)</span>
<span>Fig. 10. Contrast Feature Vectors for Pedestrian ROIs and non-Pedestrian ROIs from Sequence 3 shown in ﬁgure 17(c1) (details in section IV, table I). Circles: pedestrian ROIs. Dots: non-pedestrian ROIs. (a): 2D “upper-contrast-index” for ROIs. X axis: ROI row-edge index. Y axis: upper row-edge index. (b): 2D “lower-contrast-index” for ROIs. X axis: ROI row-edge index. Y axis: lower row-edge index.</span>
<span>To demonstrate the property of ROI contrast-feature vectors, ﬁgure 10(a) and (b) respectively plot the uppercontrast-index, i.e., “ROI row-edge index”(X axis) vs. “upper row-edge index”(Y axis), and the lower-contrastindex, i.e., “ROI row-edge index”(X axis) vs. “lower row-edge index”(Y axis), for ROIs from sequence 3 shown in ﬁgure 17(c1) (details in section IV, table I). Feature points for pedestrian ROIs and non-pedestrian ROIs are labeled by circles and dots, respectively. As expected, the “upper/lower row-edge index” for all pedestrian ROIs are not larger than 1, especially for the lower vertical neighbor regions. Among 248 pedestrian ROIs (circle-points) in ﬁgure 10(a) and (b), most have zero “upper/lower row-edge indices” except for 5 pedestrian ROIs (2.02 %) have vertical edge information underneath that leads to non-zero “lower row-edge indices” (within (0,1]), and 13 pedestrian ROIs (5.24 %) that have vertical edge information in the upper neighborhoods that leads to non-zero “upper row-edge indices” (within (0,1]). The average “upper/lower row-edge index” is 0.0166 and 0.0083. The largest “ROI row-edge index” is 1.667. On the other hand, we can see that points for non-pedestrian ROIs in the “upper-contrast-index” and the “lowercontrast-index” 2D space are much more diversiﬁed. The statistical contrast property for pedestrian/non-pedestrian ROIs demonstrates that we can identify non-pedestrian ROIs by checking their contrast index based on two conditions mentioned in section III-C, and our selected threshold is conservative. D. Multi-dimensional Classiﬁcation Feature Among three deﬁned classiﬁcation features, we can directly use 1D histogram-based or 1D inertial-based classiﬁcation to determine pedestrians by measuring the similarity between ROIs and one pedestrian template. For pedestrian ROIs,</span>
<span>13</span>
<span>the expected histogram feature index should be close to 0, and the inertial feature index should be close to 1. The farther the histogram or inertial feature of an ROI deviates from its expected value, the less likely is the ROI to be a pedestrian. Because “contrast-based non-pedestrian ROI-removal” is best at distinguishing non-pedestrian ROIs lacking in vertical contrast, the contrast-based feature has to be combined with other classiﬁcation features. Classiﬁcation results based on 1D histogram feature alone can be very close to the ideal ROC boundary for winter sequences as shown in ﬁgure16(a)(details in section IV). To improve classiﬁcation performance in complicated scenarios, we propose multi-dimensional classiﬁcation methods. We ﬁrst introduce 2D histogram/inertial-based classiﬁcation in section III-D.1 in which the inertial feature helps to remove ambiguity introduced in 1D histogram-based classiﬁcation as mentioned in section III-B.2.In section III-D.2, we introduce 3D histogram/inertial/contrast-based classiﬁcation that involves “contrast-based non-pedestrian ROI-removal” to further decrease the ambiguity associated with 2D histogram/inertial classiﬁcation. D.1 2D Histogram/Inertial-based Classiﬁcation For 2D histogram/inertial-based classiﬁcation method, the similarities between ROIs and one pedestrian template (in ﬁgure 6(a0)) are measured through 2D histogram/inertial feature vectors. The statistical distribution of 2D histogram/inertial feature vectors for all ROIs from three sequences in ﬁgure 17(a1)(b1)(c1) (details in section IV, table I) are respectively presented in ﬁgure 11(a)(b), 12(a)(b), and ﬁgure 13(b)(c). Figures for both pedestrian ROIs and nonpedestrian ROIs in the same sequences are plotted with the same axis to demonstrate the distribution diﬀerences of feature vectors. We can see that 2D feature values for all pedestrian ROIs are similar and close to their expected value [1, 0] (X axis: inertial. Y axis: histogram.) as shown in ﬁgure 11(a), 12(a), and ﬁgure 13(b). Histogram/inertial feature vectors for non-pedestrian ROIs are away from [1, 0] and much more diversiﬁed, as shown in ﬁgure 11(b), 12(b), and ﬁgure 13(c). (Non-pedestrian ROIs in ﬁgure 13(c) are after “contrast-based non-pedestrian ROI-removal.”) The comparison conﬁrms that 2D histogram/inertial-based features are eﬃcient classiﬁcation feature vectors.</span>
<span>0 2 4 6 8 10 12</span>
<span>0</span>
<span>5</span>
<span>10</span>
<span>15</span>
<span>20</span>
<span>25</span>
<span>0 2 4 6 8 10 12</span>
<span>0</span>
<span>5</span>
<span>10</span>
<span>15</span>
<span>20</span>
<span>25</span>
<span>0 2 4 6 8 10 12</span>
<span>0.2</span>
<span>0.3</span>
<span>0.4</span>
<span>0.5</span>
<span>0.6</span>
<span>0.7</span>
<span>0.8</span>
<span>0.9</span>
<span>0 2 4 6 8 10 12</span>
<span>0.2</span>
<span>0.3</span>
<span>0.4</span>
<span>0.5</span>
<span>0.6</span>
<span>0.7</span>
<span>0.8</span>
<span>0.9</span>
<span>(a) (b) (c) (d)</span>
<span>Fig. 11. 2D Feature Vectors for Pedestrian ROIs and non-Pedestrian ROIs from Sequence 1. For (a)(b): 2D inertial/histogram feature vectors for pedestrian ROIs and non-pedestrian ROIs respectively. X axis: Inertial feature. Y axis: “Histogram Diﬀerence,” for ROIs and pedestrian template in ﬁgure 6(a0). For (c)(d): 2D inertial/pixel-comparison based feature vectors for pedestrian ROIs and non-pedestrian ROIs respectively. X axis: Inertial feature. Y axis: “Image-Intensity-Diﬀerence” between ROIs and pedestrian template in ﬁgure 6(a0). In ﬁgure 11(a) and (b), histogram feature points for 19.64% of pedestrian ROIs and 16.13% of non-pedestrian ROIs overlap in their data ranges. In ﬁgure 11(c) and (d), pixel-comparison based feature points for all pedestrian ROIs and 85.74% of non-pedestrian ROIs overlap in their data ranges. The comparison between (a)(b) and (c)(d) shows the advantages of the histogram feature over the shape-dependent pixel-comparison based feature.</span>
<span>D.2 3D Histogram/Inertial/Contrast-based Classiﬁcation Our3Dhistogram/inertial-feature/contrast-basedclassiﬁcationalgorithmﬁrstcalculatesROI contrast-feature vectors for each ROI, then removes a few non-pedestrian ROIs based on two conditions mentioned in section III-C, and ﬁnally identiﬁes pedestrians among the rest of ROIs through 2D histogram/inertial based classiﬁcation. An example of the process is shown in ﬁgure 10 and ﬁgure 13 for sequence 3 shown in ﬁgure 17(c1) (details in section IV, table I). After segmentation, there are total 248 pedestrian ROIs and 854 non-pedestrian ROIs whose contrast-feature vectors are plotted in ﬁgure 10. In the process of “contrast-based non-pedestrian ROI-removal,” 284 non-pedestrian ROIs lacking in clear vertical contrast are identiﬁed and removed. The inertial vs. histogram 2D feature vectors for 284 removed non-pedestrians, 248 segmented pedestrian ROIs and 570 remained non-pedestrian ROIs are respectively plotted in ﬁgure 13(a)(b)(c). The comparison between ﬁgure 13(a)(b) shows that 2D feature points for 76.76% of removed nonpedestrian ROIs are within the data range for pedestrian ROIs. The contrast-based feature helps to remove potential ambiguity using 2D histogram/inertial-based classiﬁcation alone. Therefore, after “contrast-based non-pedestrian ROIremoval”, the percentage of segmented non-pedestrian ROIs, whose 2D feature vectors overlap with that of segmented pedestrian ROIs in 2D feature space, has dropped from 47.78% to 25.53% (as shown in ﬁgure 13(b)(c)). Thus when</span>
<span>14</span>
<span>0 1 2 3 4 5 6 7 8</span>
<span>0</span>
<span>1</span>
<span>2</span>
<span>3</span>
<span>4</span>
<span>5</span>
<span>6</span>
<span>7</span>
<span>8</span>
<span>(a) 0 1 2 3 4 5 6 7 8 0</span>
<span>1</span>
<span>2</span>
<span>3</span>
<span>4</span>
<span>5</span>
<span>6</span>
<span>7</span>
<span>8</span>
<span>(b)</span>
<span>Fig. 12. 2D Inertial/Histogram Feature Vectors for Pedestrian ROIs and non-Pedestrian ROIs from Sequence 2. X axis: Inertial feature. Y axis: Histogram feature. (a): For pedestrian ROIs. (b): For non-pedestrian ROIs.</span>
<span>detection rate is 100%, the false alarm rate can be dropped from 47.78% to 25.53% as shown ﬁgure 16(c1) and (c2), improving classiﬁcation performance.</span>
<span>0 0.2 0.4 0.6 0.8 1 1.2 1.4</span>
<span>0</span>
<span>0.5</span>
<span>1</span>
<span>1.5</span>
<span>2</span>
<span>2.5</span>
<span>3</span>
<span>3.5</span>
<span>4</span>
<span>4.5</span>
<span>0 0.2 0.4 0.6 0.8 1 1.2 1.4</span>
<span>0</span>
<span>0.5</span>
<span>1</span>
<span>1.5</span>
<span>2</span>
<span>2.5</span>
<span>3</span>
<span>3.5</span>
<span>4</span>
<span>4.5</span>
<span>0 0.2 0.4 0.6 0.8 1 1.2 1.4</span>
<span>0</span>
<span>0.5</span>
<span>1</span>
<span>1.5</span>
<span>2</span>
<span>2.5</span>
<span>3</span>
<span>3.5</span>
<span>4</span>
<span>4.5</span>
<span>(a) (b) (c)</span>
<span>Fig. 13. 2D Inertial/Histogram Feature Vectors for Pedestrian/non-pedestrian ROIs from Sequence 3 after “contrast-based non-pedestrian ROI-removal.” (a): For removed non-pedestrian ROIs. (b): For original pedestrian ROIs. (c): For remained non-pedestrian ROIs. X axis: Inertial feature. Y axis: Histogram feature.</span>
<span>E. Comparison with Conventional Classiﬁcation Feature In this section, we compare the classiﬁcation ability of two shape-independent features — histogram based and inertial based — with that of a conventional pixel-comparison based feature. For ideal classiﬁcation features, the feature points for multiple pedestrian ROIs are expected to be close to their expected values. The data ranges of feature values for both pedestrian ROIs and non-pedestrian ROIs should not overlap and are expected to be separated as far as possible. In this paper, three diﬀerent features measure the similarity between ROIs and a common pedestrian template that is deﬁned as the (in ﬁgure 6(a0)). Speciﬁcally the histogram feature and inertial feature are calculated according to the equation (5) and (6), and the pixel-comparison based feature is deﬁned as the Frobenius norm of image pixel intensity diﬀerences between ROIs and the pedestrian template. We evaluate three feature deﬁnitions for ROIs in ﬁgure 6(a1)(a2) and plot three 1D-classiﬁcation features — pixelcomparison based, histogram based, and inertial based — respectively in ﬁgure 14(a), (b), and (c). Feature points for pedestrian/non-pedestrian ROIs are labelled with circles and dotted points, respectively. We can see that the ratio of overlapped range over the data range for all non-pedestrian ROIs is respectively 0% for histogram-based method (ﬁgure 14(b)), 3.87% for inertial-based method (ﬁgure 14(c)), and 48.22% for conventional pixel-comparison based method (ﬁgure 14(a)). The comparison illustrates that pixel-comparison based features are sensitive to pose-changes in multiple pedestrian ROIs. When using only one pedestrian template, the classiﬁcation performance based on conventional pixelcomparison feature is much worse than based on 1D histogram-features or 1D inertial-features. To reach 100% pedestrian detection rate, the false alarm rate can be as high as 48.22% for conventional shape-dependent pixel-comparison feature, while it is only 0%, 3.87% for two 1D shape-independent histogram and inertial features. Histogram feature can also</span>
<span>15</span>
<span>identify the second pedestrian ROI in ﬁgure 6(a1) that contains extra background region. Figure 14(d) plots 2D inertial (X axis) vs. histogram (Y axis) feature vectors for all ROIs. The comparison between ﬁgure 14(d) and ﬁgure 14(a)-(c) shows the advantages of 2D based histogram/inertial classiﬁcation over each 1D classiﬁcation. To statistically demonstrate the above advantages, similar comparison is shown in ﬁgure 11 for a large set of ROIs from sequence 1 shown in ﬁgure 17(a1) (details in section IV, table I). Figure 11(a)(b) are inertial feature (X axis) vs. histogram feature (Y axis), and ﬁgure 11(c)(d) are inertial feature (X axis) vs. pixel-comparison based feature (Y axis). In the vertical axis of ﬁgure 11(a) and (b), histogram feature points for 19.64% of pedestrian ROIs and 16.13% of nonpedestrian ROIs overlap in their data ranges. In the vertical axis of ﬁgure 11(c) and (d), pixel-comparison based feature points for all pedestrian ROIs and 85.33% of non-pedestrian ROIs overlap. In 2D inertial vs. histogram space, the ratios of overlapped range over the data range for all pedestrian ROIs and for all non-pedestrian ROIs are respectively 12.13% and 16.31%. As expected, the histogram features provide better classiﬁcation performance than the shape-dependent pixel-comparison based feature. Classiﬁcation based on both histogram and inertial further improve performance. More results will be shown in the next section to demonstrate the advantages of multi-dimensional-classiﬁcation as shown in ﬁgure16.</span>
<span>(a)</span>
<span>(b)</span>
<span>(c)</span>
<span>(d)</span>
<span>Fig. 14. Classiﬁcation Ability Comparison. For (a)-(d): feature points of diﬀerent deﬁnitions that measure the similarity between ROIs in ﬁgure 6(a1)(a2) and the default pedestrian template shown in ﬁgure 6(a0). Circles and dots respectively denote pedestrian ROIs and nonpedestrian ROIs. (a): Conventional 1D Pixel-Comparison based Feature. (b): 1D Histogram-based Feature. (c): 1D Inertial-based Feature. (d): 2D Histogram/Inertial Feature. For (a)-(c): Bottom/Top lines: data ranges for pedestrian/non-pedestrian ROIs. X axis: feature value. The Y axis is only used to separate two lines. The overlap percentage between them over non-pedestrian data range: 48.22%(a), 0%(b), 3.87%(c). For (d): X axis: inertial value. Y axis: histogram diﬀerence value.</span>
<span>IV. Performance Evaluation Up to now, we have presented our segmentation and classiﬁcation algorithms. In real-world applications, both brightness-based and bodyline-based segmentation will be applied, and all segmented ROIs will be sent to multidimensional histogram-inertial-contrast-based classiﬁers for reliability. In this paper, for the purpose of performance evaluation, we apply diﬀerent combinations of segmentation/classiﬁcation algorithms to detect pedestrians in three typical scenarios: winter driving (Sequence 1, ﬁgure 17(a1)), summer suburban driving (Sequence 2, ﬁgure 17(b1)), and summer urban driving (Sequence 3, ﬁgure 17(c1)). From sequence 1 to sequence 3, driving complexity increases. For sequence 1 and 2, even the simpliﬁed version of our pedestrian detection (segmentation/classiﬁcation) algorithm has improved the current detection performance as shown in ﬁgure 3, which demonstrates the potential and eﬀectiveness of our algorithms. In this section we ﬁrst introduce the basic information for the three sequences as summarized in table I. Then we present segmentation results in section IV-B) as summarized in table II, and classiﬁcation results in section IV-C as summarized in table III. Pedestrian detection examples for three sequences are shown in ﬁgure 17(a), (b), and (c). The initial ROIs (after segmentation) and ﬁnal detection results (after classiﬁcation) are highlighted. A. Test Sequences The examples of pedestrian appearances for three sequences can be seen respectively in ﬁgure 6 (sequence 1) and ﬁgure 7(a1)(a2) (sequence 2 and 3). All these video sequences were taken by Toyota R&amp;D labs using a far-infrared</span>
<span>16</span>
<span>TABLE I Sequence Information for three Examples Image Frame Duration Ped. Pedes Size Size No. Sequence Infor. Figure # (sec) # Range Change Complexity 1st Winter Fig.17(a1), 6(a1) 240 40 331 [83×182, 36×96] 4.3 Low 2nd Summer, Suburban Fig.17(b1), 7(a1) 289 48.1 176 [9×18, 30×68] 12.6 Medium 3rd Summer, Urban Fig.17(c1), 7(a2) 248 41.3 248 [9×17, 33×67] 14.5 High</span>
<span>TABLE II Segmentation Algorithms &amp; Performance for three Examples Segmentation Image ROI # missed- Eval. Accuracy Eﬃciency No. Method Figure ped./non-ped. ped. # Figure Avg. [range] Avg. [range] 1st Brightness Fig.17(a) [331, 750] 0 Fig.15(a2) 95.23% [0.8058, 1] 85.84% [0.4972, 1] 2nd Brightness Fig.17(b) [176, 909] 0 Fig.15(b2) 74.99% [0.4648 1] 89.36% [0.2375 1] 3rd Bodyline Fig.17(c) [248, 854] 0 Fig.15(c2) 90.11% [0.5847, 1] 89.08% [0.5278, 1]</span>
<span>camera with the wavelength band 8 to 14 um at a frame rate of 6 fps, i.e., 6 frames per second. The frame number and duration for sequence 1, 2, and 3 are respectively 240 frames (40 seconds), 289 frames (48.1 seconds), and 248 frames (41.3 seconds). Sequences recorded the whole process: pedestrians ﬁrst appeared far away with small image patches (as in the ﬁrst column in ﬁgure 17), then became closer and larger, until they ﬁnally disappeared from roadsides (as in the last column in ﬁgure 17). The total number of pedestrians inside three sequences and the variation ranges of pedestrian sizes are listed in table I. We can see that within these sequences, the sizes of pedestrian appearance change signiﬁcantly from as small as 9×17 (in sequence 3) to as large as 83×182 (in sequence 1), 99 times larger. In the middle of sequence 2, a pedestrian was occluded by a truck in 21 frames. Sequence 2 also recorded 92 additional frames after pedestrians disappeared. We expect no false-alarms in these “empty” frames if our proposed shape-independent segmentation/classiﬁcation works. B. Segmentation Performance To demonstrate the segmentation performance, we apply “brightness-based” segmentation to sequence 1 and 2 (winter and summer suburban driving) and “bodyline-based” segmentation to sequence 3 (summer urban driving). To evaluate segmentation quality based on our newly proposed index, i.e., side-accuracy and side-eﬃciency, we have labeled the ground truth of pedestrians’ positions (in rectangular regions) within all sequence frames. The closer the two segmentation indices are to 100%, the more accurate and eﬃcient is the performance. The examples of initial segmented ROIs are highlighted in the second rows of ﬁgure 17(a)(b)(c), which include both pedestrians and false alarms that would be removed in classiﬁcation procedures. Table II lists the number of segmented pedestrian/non-pedestrianROIsandmissedpedestrians, andsummarizessomethemeanandrangeforbothperformance evaluation indices, segmentation side-accuracy and side-eﬃciency. Figure 15 plots segmentation side-accuracy (X axis) vs. segmentation side-eﬃciency (Y axis) for each frame as a point in 2D space. About 90.42% of sequence 1 frames and 94.97% of sequence 3 frames have both accuracy and eﬃciency indices larger than 70%. For sequence 2, 93.18% of frames have accuracy and eﬃciency indices larger than 50% and 70% respectively. In a total of 777 frames in 3 sequences, only 9 frames’ (1.16%) segmentation side-eﬃciency is less than 50% and 4 frames’ (0.51%) segmentation side-accuracy is less than 50%, most of which are from brightness-based results for sequence 2 whose accuracy performance is less accurate than the other two sequences. Some ROIs capture only partial pedestrians as shown in ﬁgure 15(b). Full segmentation algorithms based on both brightness/bodyline will improve segmentation performance.</span>
<span>TABLE III Classification Algorithms &amp; Performance for three Examples Classiﬁcation Image Feature ROC No. Method Figure Vector Fig. Figure 1st 1D Inertial or Histogram Fig.17(a3) Fig.11 Fig.16(a), 3(a) 2nd 2D Inertial/Histogram Fig.17(b3) Fig.12 Fig.16(b), 3(a) 3rd 3D Inertial/Histogram/Contrast Fig.17(c3) Fig.13, Fig.10 Fig.16(c), 3(a)</span>
<span>17</span>
<span>0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1</span>
<span>0</span>
<span>0.1</span>
<span>0.2</span>
<span>0.3</span>
<span>0.4</span>
<span>0.5</span>
<span>0.6</span>
<span>0.7</span>
<span>0.8</span>
<span>0.9</span>
<span>1</span>
<span>0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1</span>
<span>0</span>
<span>0.1</span>
<span>0.2</span>
<span>0.3</span>
<span>0.4</span>
<span>0.5</span>
<span>0.6</span>
<span>0.7</span>
<span>0.8</span>
<span>0.9</span>
<span>1</span>
<span>0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1</span>
<span>0</span>
<span>0.1</span>
<span>0.2</span>
<span>0.3</span>
<span>0.4</span>
<span>0.5</span>
<span>0.6</span>
<span>0.7</span>
<span>0.8</span>
<span>0.9</span>
<span>1</span>
<span>Fig. 15. Segmentation Evaluation for 3 Sample Sequences. Detection Accuracy vs. Eﬃciency. X axis: frame segmentation side-accuracy. Y axis: frame segmentation side-eﬃciency. (a): Sequence 1. (b): Sequence 2. (c): Sequence 3.</span>
<span>C. Classiﬁcation Performance The classiﬁcation algorithms for three sequences are respectively 1D histogram-based, 2D histogram/inertial-based, and 3D histogram/inertial/contrast-based. For three sequences, their classiﬁcation performance indices — ROC boundary as deﬁned in I-A — are respectively plotted with solid lines in ﬁgure 16 (a), (b), and (c2). All ROC curves or ROC boundaries are close to the ideal ROC boundary shown in ﬁgure 2(b), and present high detection rate and small false-alarm rate. The examples of classiﬁcation results are highlighted in the third rows of ﬁgure 17(a)(b)(c), where some false alarms are removed. Figure 3(a) compares the classiﬁcation results of marked points in ﬁgure 16(a)(b)(c2), and other available published results in diﬀerent seasons by plotting their frame false-alarm/detection rate index points in 2D space. For winter driving, we mark ROI curve points in ﬁgure 16(a) whose false-alarm rate is similar to other published winter result[18] and notice that our detection rate is higher. For summer driving, we mark ROI curve points in ﬁgure 16(b)(c2) whose detection rates are similar to other published summer results[12] and notice that our false-alarm rates are smaller. Figure 3(a) shows that our classiﬁcation index points are at the upper and left regions of other classiﬁcation results and present higher detection rate with less false alarm. The performance of 1D histogram-based classiﬁcation performance (ﬁgure 16(a)) is reliable for winter driving sequence 1, which partially beneﬁts from accurate segmentation performance as shown in ﬁgure 15. In general, 1D-feature-based classiﬁcation performance is limited for summer driving as shown in ﬁgure 16(b) (sequence 2) and (c) (sequence 3) where dashed and dotted lines are respectively for 1D-histogram-based and 1D-inertial-based classiﬁcation. It is because of more complex image property and more image “noise” for summer images. Besides, brightness-based segmentation accuracy for summer suburban driving sequence 2 is relatively less accurate than for winter driving, which adds to classiﬁcation diﬃculties. Fusing histogram-based and inertial-based classiﬁcation substantially improves classiﬁcation performance as shown by the ROC-curve-comparison between solid lines (for 2D histogram/inertial-based classiﬁcation) and dashed/dotted lines (for 1D histogram-based and 1D inertial-based classiﬁcation) in ﬁgure 16(b) (sequence 2) and (c) (sequence 3). Final 2D histogram/inertial classiﬁcation performance for sequence 2 reﬂects its eﬀectiveness for summer suburban driving. The contrast classiﬁcation feature helps to remove the ambiguity. Figure 16(c1) and ﬁgure 16(c2) respectively show the diﬀerent classiﬁcation results before and after “contrast-based non-pedestrian ROI-removal.” The advantage of 3D histogram/inertial/contrast-based classiﬁcation over 2D histogram/inertial-based classiﬁcation can be seen from the diﬀerence between solid lines in ﬁgure 16(c1) and (c2). The comparison between dashed (or dotted) lines in ﬁgure 16(c1) and (c2) shows the advantage of 2D histogram/contrast-based (or 2D inertial/contrast-based) classiﬁcation over 1D histogram (or 1D inertial) based classiﬁcation. For three sequences, we only apply 3D histogram/inertial/contrast-based classiﬁcation to the most complicated sequence 3 as shown in ﬁgure 16(c2) since 1D or 2D classiﬁcation has already provided reliable results for the rest of sequences. In sum, the segmentation performance illustrated in ﬁgure 15 shows that segmented pedestrian regions are relatively accurate and eﬃcient. The classiﬁcation performance illustrated in ﬁgure 16 show that most false alarms are removed.</span>
<span>18</span>
<span>(a) (b)</span>
<span>(c1) (c2)</span>
<span>Fig. 16. Classiﬁcation Performance Evaluation for Three Sample Sequences. X axis: false alarm rate. Y axis: detection rate. (a): Sequence 1: ROC for histogram-based classiﬁcation. (b): Sequence 2: Histogram/inertial-based classiﬁcation. Dashed line: ROC for inertial-based classiﬁcation. Dotted line: ROC for histogram-based classiﬁcation results. Star points: Histogram/inertial-based classiﬁcation result. (c1) Sequence 3: ROC for histogram/inertial-based classiﬁcation. Dashed line: ROC for inertial-based classiﬁcation. Dotted line: ROC for histogram-based classiﬁcation results. Star points: Histogram/inertial-based classiﬁcation result. (c2) Sequence 3: ROC for histogram/inertial/contrast-based classiﬁcation. Dashed line: ROC for inertial/contrast-based classiﬁcation. Dotted line: ROC for histogram/contrast-based classiﬁcation results. Star points: Histogram/inertial/contrast-based classiﬁcation result.</span>
<span>V. Summary and Future work This paper presents new methods for detecting pedestrians in far-infrared images in order to improve night driving safety. To reliably detect pedestrians with arbitrary poses, we introduce a new “Shape-Independent” detection method that stands in contrast to conventional shape-based detection methods to improve performance. In summary, there are two main contributions: 1. We propose an original “horizontal-ﬁrst, vertical-second” segmentation scheme that ﬁrst divides infrared images into several vertical image stripes, and then searches for pedestrians only within these image stripes. The algorithm can automatically estimate the size of pedestrian regions based on properties of the bright-pixel vertical-projection curves and pedestrians’ horizontal contrast. Thus, we avoid brute-force searching over the entire images. Our algorithm has wide applicability, since it only assumes that there is some local contrast between the image of a pedestrian and its surround and does not make other assumptions about the driving environment. 2. We have deﬁned unique new shape-independent multi-dimensional classiﬁcation features, speciﬁcally, histogram-, inertial-, and contrast-based features, and also demonstrated the similarities of these features among pedestrian image regions with diﬀerent poses, as well as the diﬀerences of these features between pedestrian and non-pedestrian ROIs. The “histogram variation curve” for all pedestrian regions resembles a Gaussian shape of zero mean, while the distribution of inertial-features resembles a Rayleigh distribution with expected value 1. Contrast-features — the ROI row-edge indices, and the upper/lower row-edge indices — for pedestrian ROIs fall within speciﬁc data regions. In this way, pedestrians can be identiﬁed by comparing the similarity of these features derived from segmented ROIs with those of a pedestrian template. Only one generic pedestrian template is needed. In contrast, traditional image pixel-comparison based classiﬁcation is shape-dependent and so multiple pedestrian templates are necessary to deal with pedestrians in diﬀerent poses. On the whole, though the proposed pedestrian detection is by no means perfect for real world applications and we still</span>
<span>19</span>
<span>need to further improve the detection performance, it has made much progress considering the current research stages, and present encouraging results. Shape-independent features are more robust with respect to pedestrian pose-changes than traditional shape-dependent features. Also, our segmentation and classiﬁcation processes collaborate with one another. Initial horizontal segmentation and bodyline searching improves the segmentation accuracy and eﬃciency, and fewer segmentation errors lead to fewer classiﬁcation errors. At the same time, the computational load is low because our segmentation process avoids brute-force searching over the whole image and the classiﬁcation process avoids the need for comparison with multiple pedestrian templates. The proposed new statistical features can be fused with other general pedestrian detection features for multi-dimensional feature-based detection to further improve reliability and speed.</span></p></div>
</body>
</html>